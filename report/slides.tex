\documentclass{beamer}
% \usepackage[TU]{fontenc}
\usepackage{lmodern} % load a font with all the characters
% \usepackage{unicode-math}

\setlength{\parskip}{\baselineskip} 

% Essential packages
\usepackage[english,danish]{babel}
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\usepackage[skins,minted,breakable]{tcolorbox}
\usepackage{booktabs}

% Math packages
\usepackage{amsmath,amsfonts,amssymb,amsthm}

% Programming packages
\usepackage{listings,newtxtt}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{courier}

% Layout packages
\usepackage{csquotes}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{lipsum}
\usepackage{subcaption}
\usepackage[style=numeric-comp,maxbibnames=99]{biblatex}
% \usepackage[hyphenbreaks]{breakurl}
% \usepackage[htt]{hyphenat}
\usepackage{array}
\usepackage{colortbl}
\usepackage{outlines}

\usepackage{tikz}  
\usetikzlibrary{chains, positioning, arrows.meta, bending, shapes.arrows}

\addbibresource{bibliography.bib}
\addbibresource{iacr.bib}

\newcommand{\eff}{\mathbb{F}} % Fancy F
\newcommand{\pp}{\texttt{++}} % ++ operator
\newcommand{\rightshift}{\texttt{ >> }} 
\newcommand{\td}[1]{\todo[inline]{#1}} % Todo notes

\title{Implementation and Evaluation of Dinur's MQ Solver Algorithm}
\author{Author: Mikkel Juul Vestergaard\\Supervisor: Associate Professor Ruben Niederhagen}
\institute{Department of Mathematics and Computer Science\\University of Southern Denmark}
\date{Thesis Defence, 2023}

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Table of Contents}
    \tableofcontents[hideallsubsections,sectionstyle=show/shaded]
  \end{frame}
}

\begin{document}

\frame{\titlepage}

\section{Preface}
\begin{frame}
    \frametitle{The goals of this thesis.}
    \enquote{\textelp{} I will provide an environment for solving multivariate polynomial systems using the algorithm proposed Itai Dinur \textelp{} based on traditional desktop computers, HPC hosts or a subset of these. \textelp{} the implementation of the algorithm in question will have host-specific traits \textelp{} I will further produce a report, shedding light on the practical performance of the algorithm from [1].}\footnote{[1] refers to the paper by Itai Dinur, introducing his polynomial-method algorithm.}
\end{frame}

\section{Introduction}
\begin{frame}
    \frametitle{Polynomials?}
    A mathematical expression or function in some variable(s), coefficients and declared through the operators:
    \begin{itemize}
        \item Addition.
        \item Subtraction.
        \item Multiplication.
        \item Exponentiation using non-negative integers.
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Polynomials.}
    Some simple polynomials as mathematical functions:
    \begin{equation*}
        \begin{split}
            p(x) &= 4x + 1\\
            f(x) &= 2x^2 + 3
        \end{split}
    \end{equation*}
    and expressions:
    \begin{equation*}
        \begin{split}
            &x^{10}\\
            &x^5 + 10x^3
        \end{split}
    \end{equation*}

\end{frame}

\begin{frame}
    \frametitle{Multivariate polynomials.}
    So, what are these \textit{multivariate} polynomials?
    
    \pause 

    Well, just polynomials in \textit{multiple} variables!

    \pause 

    Typically we denote these variables by subscript (of course starting from $0$).

    E.g. $x_0, x_1, \dots x_{n - 1}$, however, we can still of course call them $x, y, z$, etc.

    \pause 

    For a notational shorthand, 
    $$
        \mathbf{x} = (x_0, x_1, \dots x_{n - 1})
    $$ 
    for $n$ variables, and likewise for variables denoted by $y$s and $z$s.
\end{frame}

\begin{frame}
    \frametitle{Multivariate polynomials.}
    So how do these look?

    \pause 

    Quite simply, they look like the polynomials we just saw but with more variables!

    \pause 
    For example:
    \begin{equation*}
        \begin{split}
            p_0(x_0, x_1, x_2, x_3) &= 5x_3^4 + x_0^2 + 4x_1 + 10\\
            p_1(x_0, x_1, x_2, x_3) &= 3x_1x_2x_3 + 2x_0^2
        \end{split}
    \end{equation*}
\end{frame}

\begin{frame}
    \frametitle{Boolean multivariate polynomials!}
    But in fact, sometimes we want to restrict multivariate polynomials. How may we do that?

    \pause 

    We may restrict the \textit{coefficients} and \textit{variable} assignments.
    
    In this case, the coefficients are restricted to $0$s and $1$s, and the same goes for the assignments we can make to the variables!

    \pause 

    I.e. we restrict the multivariate polynomials to look like 
    \begin{equation*}
        \begin{split}
            &x_0x_1 + x_0\\
            &x_0^2 + x_1x_2
        \end{split}
    \end{equation*}
    \pause 
    
    Polynomials of this sort are often called \textit{boolean} polynomials, or in this case \textit{boolean multivariate polynomials}.
\end{frame}

\begin{frame}
    \frametitle{Boolean multivariate polynomials! (Extended)}
    The operations of multiplication and addition 
\end{frame}


\begin{frame}
    \frametitle{What is the ''MQ-problem''?}
\end{frame}

\begin{frame}
    \frametitle{Dinur's algorithm.}
\end{frame}

\section{Contributions}
\begin{frame}
    \frametitle{}
\end{frame}

% \section{Preliminaries}
\begin{frame}
    \frametitle{Rings of polynomials?}
    \begin{outline}
        \1 \textit{System} of multivariate polynomials?
            \2 Much like the linear systems you know from linear algebra!
            \2 Using the polynomial ring $R[x]$ we may combine multiple polynomials into a \textit{system}.
        \1 Alright, why bother?
            \2 Solving systems of multivariate polynomials ($R[x_0,\dots x_{n - 1}]$) is NP-hard!
                \3 We often denote $MP$ as the problem of solving multivariate polynomials of degree $d$ in $n$ variables and $m$ polynomials.
            \2 A commonly used version of the $MP$ problem is the $MQ$-problem.
                \3 This is the problem of solving systems of $m$ quadratic polynomials in $n$ variables.
                \3 This is NP-complete
                \3 Much modern cryptography is based on the hardness of this problem
    \end{outline}
\end{frame}

\subsection{Typical Tricks}
\begin{frame}
    \frametitle{Bit-slicing and linearization}
    Typically, when we implement procedures handling systems of multivariate polynomials over $\mathbb{F}_2[x_0,\dots x_{n - 1}]$ we use \textit{bit-slicing}: \pause

    Consider the polynomials 
    \begin{equation*}
        \begin{split}
            p_0(x_0, x_1) &= 1 + x_1 + x_0x_1,\\
            p_1(x_0, x_1) &= x_0 + x_1.
        \end{split}
    \end{equation*}
    Noticed how the coefficients can be represented as bits in a computer? Let us use this fact!

    We can combine $p_0$ and $p_1$ into a list of integers:
    
    \pause
    $$
        p = [01_2, 10_2, 11_2, 01_2].
    $$
    If we wish to compute something on both polynomials, we can now simply use bit-wise operators \texttt{xor} for $+$ and \texttt{and} for $\cdot$.
\end{frame}

\begin{frame}
    \frametitle{Bit-slicing and linearization}
    For the sake of the algorithms in this talk, \textit{linearization} is important! \pause

    For the polynomial (over $\mathbb{F}_2[x_0,x_1]$) 
    $$
        p(x_0, x_1) = 1 + x_0 + x_0x_1 + x_1^2, 
    $$
    we may \textit{remove} the quadratic term and \textit{add} a linear term instead. This yields the following polynomial:
    $$
        p(x_0, x_1) = 1 + x_0 + x_1 + x_0x_1,
    $$
    where \textit{no quadratic terms exist}!
\end{frame}

\subsection{Möbius Transform}
\begin{frame}
    \frametitle{Möbius transforms}
    \textbf{Not to be confused with a similarly named transform in the realm of complex analysis and geometry!} 
    
    \pause

    The Möbius transform can be used to obtain a \textit{boolean polynomial} (or polynomial in $\mathbb{F}_2[x_0,\dots x_{n - 1}]$) from a truth table! (see \cite{joux2009algorithmic})
    
    \pause 

    From the \textit{truth-table} of a boolean function, we may get the ANF of a polynomial:
    $$
        f(x_0, \dots x_{n - 1}) = \bigoplus_{(a_0,\dots a_{n - 1}) \in \mathbb{F}_2^n} g(a_0, \dots a_{n - 1}) \prod_i x_i^{a_i}
    $$

    This is quite cool as we can interpolate boolean polynomials from a truth-table!
    
    \pause
    
    Plus, we can do the inverse as well!
\end{frame}

\begin{frame}
    \frametitle{Gray codes}
    Gray code or Reflected Binary Code, an ordering of the binary numeral system in which every subsequent value only differs in \textit{one} bit.

    \begin{center}
        \begin{tabular}{||c|c|c||}
            \hline
            Index & Binary & Gray \\
            \hline
            0 & $000_2$ & $000_2$\\
            1 & $001_2$ & $001_2$\\
            2 & $010_2$ & $011_2$\\
            3 & $011_2$ & $010_2$\\
            4 & $100_2$ & $110_2$\\
            5 & $101_2$ & $111_2$\\
            6 & $110_2$ & $101_2$\\
            7 & $111_2$ & $100_2$\\
            \hline
        \end{tabular}
    \end{center}
\end{frame}

\subsection{Fast Exhaustive Search}
\begin{frame}
    \frametitle{Fast Exhaustive Search for polynomials in $\mathbb{F}_2$}
    Say we have some polynomial $p$ in the ring $\mathbb{F}_2[x_0,\dots x_{n-1}$].
    
    \pause

    We want to find all assignments of $x_0, \dots x_{n - 1}$ that evalute to 0 on $p$ $\implies$ we want to do an exhaustive search!

    But we need to do $2^n$ iterations, fully evaluating a polynomial in each? Can we do this faster, concretely?
\end{frame}

\begin{frame}
    \frametitle{Derivatives in finite fields}
    Instead of fully evaluating the polynomial $p$ at each input, why don't we use derivatives and Gray codes? 
    
    \pause 

    Let us evaluate in Gray code order (for $n = 3$):
    \begin{equation*}
        \begin{split}
            p(0,&0,0)\\
            p(1,&0,0)\\
            p(1,&1,0)\\
            p(0,&1,0)\\
            &\vdots\\
            p(0,&0,1)
        \end{split}
    \end{equation*}
\end{frame}

\begin{frame}
    \frametitle{Derivatives in finite fields}
    Now, between each of these evaluations, only \textit{one} variable changes its value, e.g. $001_2 \rightarrow 011_2$ means that only $x_1$ changes value

    \pause

    Now, computing $p(\mathbf{x})$ when only variable $x_i \in \mathbf{x}$ changes value can be done by adding 
    $$
        \frac{\partial p}{\partial x_i}(\mathbf{x}),
    $$
    to the previous evaluation of $p(\mathbf{x})$. 

    \pause
    
    Further, we can treat the partial derivative above as a polynomial itself, applying this idea recursively.
\end{frame}

\begin{frame}
    \frametitle{Derivatives in finite fields}
    Generally, we may compute the $j$-order partial derivative recursively, by 
    $$
        \frac{\partial^{j - 1} p}{\partial x_{\beta_0} \dots \partial x_{\beta_{j - 2}}}(g_i) = D_{x_{\beta_0} \dots x_{\beta_{j - 2}}} + \frac{\partial^j p}{\partial x_{\beta_0} \dots \partial x_{\beta_{j - 1}}}(g_i).
    $$
    where $\mathbf{g}_i$ is Gray code encoding of $\mathbf{x}$ and $\beta_0, \dots \beta_{j - 1}$ encode the indices of all 1-bits in $\mathbf{x}$, limited by $j < d$.
    
    Above, $D_{x_{\beta_0} \dots x_{\beta_{j - 1}}}$ is the \textit{stored} previous evaluation of 
\end{frame}

\begin{frame}
    \frametitle{Derivatives in finite fields}
    Using these ideas, the authors of \cite{ches-2010-23990} managed to create an exhaustive search algorithm that uses 
    $$
        2d \cdot \log n \cdot 2^n
    $$
    \textit{bit operations} (not asymptotic) to find all solutions (common zeros) of a polynomial system. 

    The tradeoff is the storage of intermediate derivatives throughout the algorithm.

    In comparison, a naive full evaluation would require 
    $$
        2^n \cdot \sum_{i = 0}^{d} \binom{n}{d}
    $$
    \textit{bit operations}, at least.
\end{frame}

\begin{frame}
    \frametitle{Example for the quadratic case}
    \begin{algorithm}[H]
        \DontPrintSemicolon
        \SetAlgoLined
        \caption{EVAL($p$, $n$)}\label{alg:fes_eval}
        \KwIn{The polynomial(s) $p$, and the amount of variables $n$}
        \KwResult{List of common zeroes of $p$}
        $state \gets \text{INIT($p$, $n$)}$\;
        \If{$state.y = 0$}{
            Add $state.y$  to list of common zeroes\;
        }
        \While{$state.i < 2^n$}{
            STEP($state$)\;
            \If{$state.y = 0$}{
                Add $state.y$  to list of common zeroes\;
            }
        }
        \Return List of common zeroes
    \end{algorithm}
\end{frame}

\begin{frame}
    \frametitle{Example for the quadratic case}
    \begin{algorithm}[H]
        \DontPrintSemicolon
        \SetAlgoLined
        \caption{STEP($state$)}\label{alg:fes_step}
        \KwIn{The datastructure to store derivative tables, counter values, and the evaluation, $state.y$.}
        $state.i \gets state.i + 1$\;
        $k1 \gets \text{BIT}_1(state.i)$\;
        $k2 \gets \text{BIT}_2(state.i)$\;
        \If{k2 exists in state.i}{
            $s.d'[k_1] \gets s.d'[k1] \oplus s.d''[k1,k2]$\;
        }
        $s.y \gets s.y \oplus s.d'[k1]$\;
    \end{algorithm}
\end{frame}

\section{Dinur's Polynomial-Method Solver}

\subsection{The general idea}
\begin{frame}
    \frametitle{Dinur's solver}
    In \cite{eurocrypt-2021-30841}, Dinur proposed an algorithm for $MP$ problems with \textbf{exponential speedup} over exhaustive search.

    The algorithm is based on the \textit{polynomial-method}, which is borrowed from circuit complexity.

    To the best of my knowledge (and my supervisors), this algorithm has not been tested in practice.

    It borrows ideas from other algorithms based on the same method.
\end{frame}

\begin{frame}
    \frametitle{The Polynomial method}
    Algorithms based on the polynomial method solve systems 
    $$
        P = \{p_j(\mathbf{x})\}^{m - 1}_{j = 0}
    $$ 
    by considering constructing a polynomial 
    $$
        F(\mathbf{x}) = (1 + p_0(\mathbf{x}))(1 + p_1(\mathbf{x})) \dots (1 + p_{m - 1}(\mathbf{x}))
    $$
    with multiplication and addition over $\eff_2$. 
\end{frame}

\begin{frame}
    \frametitle{The Polynomial method}
    However, since $F$ may have a rather high degree ($m \cdot d$), the methods typically instead \textit{generate} a related system, 
    $$
        \Tilde{P}_k \gets \left\{r_i(\mathbf{x}) = \sum_{j = 0}^{m-1}A_{i,j} \cdot p_j(\mathbf{x})\right\}^{\ell - 1}_{i = 0},
    $$
    with a random matrix $A$ of rank $\ell$, which allows for constructing the \textit{probabilistic polynomial}
    $$
        \Tilde{F}(\mathbf{x}) = (1 + r_0(\mathbf{x}))(1 + r_1(\mathbf{x})) \dots (1 + r_{\ell - 1}(\mathbf{x})) 
    $$
    of smaller degree ($d \cdot \ell$), assuming one chooses an $\ell < m$.
\end{frame}

\begin{frame}
    \frametitle{Isolating solutions}
    \begin{block}{Variable partition}
        For the variables $\mathbf{x} = (x_0, \dots x_{n - 1})$, we make the partition $\mathbf{x} = (\mathbf{y}, \mathbf{z}) = (y_0, \dots y_{n - n_1 - 1}, z_{0}, \dots z_{n_1 - 1})$ using some parameter $n_1$.
    \end{block}

    \begin{alertblock}{Isolated solutions}
        A solution $\hat{\mathbf{x}} = (\hat{\mathbf{y}},\hat{\mathbf{z}})$ to the system $P = \{p_j(\mathbf{y}, \mathbf{z})\}^m_{j = 1}$ is called \textit{isolated} (with respect to the variable partitition $(\mathbf{y}, \mathbf{z})$) if for all $\hat{\mathbf{z}}'=\hat{\mathbf{z}}$, $(\hat{\mathbf{y}}, \hat{\mathbf{z}}')$ is not a solution to $P$.
    \end{alertblock}
    What if we, instead of searching for \textit{any} solution, search for an isolated solution? 
\end{frame}

\begin{frame}
    \frametitle{Recovering bits}
        By partitioning the variables using the parameter $n_1$, we are able to \textit{enumerate} all isolated solutions of the partition using the polynomials
        $$
            U_0(\mathbf{y}) = \sum_{\hat{\mathbf{z}} \in \{0,1\}^{n_1}} \Tilde{F}(\mathbf{y},\hat{\mathbf{z}}), \text{ and } U_i(\mathbf{y}) = \sum_{\hat{\mathbf{z}}\in \{0,1\}^{n_1 - 1}} \Tilde{F}_{z_i \leftarrow 0}(\mathbf{y}, \hat{\mathbf{z}}),
        $$
        for $i = 1, \dots n_1$. \pause

        Using the polynomials above, we know that $U_0(\mathbf{y}) = 1$ $\implies$ $(\hat{\mathbf{y}}, \hat{\mathbf{z}})$ is an isolated solution for $P$, and where $U_i(\hat{\mathbf{y}}) = \hat{z_i} + 1$.
\end{frame}

\begin{frame}
    \frametitle{Interpolation}
    Using the $U$ polynomials can help reduce the space of solutions we must search!

    \pause

    Since $\Tilde{F}$ is of a lower degree than $F$, we may now \textit{interpolate} the $U$ polynomials directly, never actually interpolating $\Tilde{F}$.

    \pause 

    We may even interpolate the $U$ polynomials using the solutions to $\Tilde{P}$ in the set 
    $$
        \left\{ W_{w + 1}^{n - n_1} \times \{0,1\}^{n_1} \right\}
    $$
    instead of the full set of $2^{n - n_1}$ solutions per $U$ polynomial. 
    
    \pause

    These solutions are computed using FES!
\end{frame}

\section{FES-Based Recovery of Solutions}
\begin{frame}
    \frametitle{Alternatives to the Möbius transform?}

    \pause 

    What if we \textit{backtrack} using the ideas of the Fast Exhaustive Search?

    \pause

    That is, normally in FES, we would compute 
    $$
        \frac{\partial^{j - 1} p}{\partial x_{\beta_0} \dots \partial x_{\beta_{j - 2}}}(g_i) = D_{x_{\beta_0} \dots x_{\beta_{j - 2}}} + \frac{\partial^j p}{\partial x_{\beta_0} \dots \partial x_{\beta_{j - 1}}}(g_i),
    $$
    where $D$ stores the previous evaluation of $\frac{\partial^{j - 1} p}{\partial x_{\beta_0} \dots \partial x_{\beta_{j - 2}}}$.
    
    \pause

    But now, we can instead compute 
    $$
        \frac{\partial^j p}{\partial x_{\beta_0} \dots \partial x_{\beta_{j - 1}}}(g_i) = \frac{\partial^{j - 1} p}{\partial x_{\beta_0} \dots \partial x_{\beta_{j - 2}}}(g_i) - D_{x_{\beta_0} \dots x_{\beta_{j - 2}}}
    $$
    and \textit{fill} the derivative table entries.
\end{frame}

\begin{frame}
    \frametitle{Alternatives to the Möbius transform?}
    Using this idea, we can construct a procedure that interpolates \textit{and} evaluates, mimicking the two separate calls to the Möbius transform.

    \begin{outline}
        \1 Go through all $2^{n - n_1}$ values of the $\mathbf{y}$ bits.
            \2 If the hamming weight of the $\mathbf{y}$ bits is larger than $d_{\Tilde{F}} - n_1 + 1$.
                \3 We have already interpolated the desired derivative table entries, and may now evaluate new points.
            \2 Else; if it is smaller.
                \3 Interpolate derivative table entries by \textit{backtracking} and store them.
    \end{outline}
\end{frame}

\section{Optimizing}
    \begin{frame}
        \frametitle{Storing solutions}
        Storing $2^{n - n_1}$ solutions in each round gets quite cumbersome!
        
        \pause 

        For the smallest case, we would need to store 
        $$
            4 \cdot 8 \cdot 2^{n - n_1}
        $$
        bytes using the proposed method! What if we instead bit-slice the $U$ polynomial evaluations?

        \pause 

        Now we need $4\cdot 2^{n - n_1}$ bytes, i.e. we saved a factor 8 in storage!
    \end{frame}


    \begin{frame}
        \frametitle{Better storage?}
        What if we store solutions sequentially, combining the $\mathbf{y}$ and $\mathbf{z}$ bits into an integer?
        
        \pause 

        As we are going to find 
        $$
            2^{n - 2n_1}
        $$
        candidate solutions, in expectation, we can save quite a large amount by storing sequentially!

        This does require some amount of computation extra since we cannot simply make a dictionary lookup.
    \end{frame}

    \begin{frame}
        \frametitle{AVX...?}
        Now, what about AVX? 
        
        \pause

        Recall that many modern x86 CPUs have 128- and 256-bit AVX registers. Can we use this to parallelize the process?

        \pause

        Yes! By \textit{partially evaluating} the polynomials!
    \end{frame}

    \begin{frame}
        \frametitle{AVX... and partial evaluation!}
        Consider the polynomial,
        $$
            p(x_0, x_1, x_2, x_3) = 1 + x_0 + x_2 + x_0x_2 + x_1x_3 + x_2x_3.
        $$
        What if we assign values to \textit{only} the last two variables, $x_2$ and $x_3$? 

        \pause 

        Then we would get
        \begin{equation*}
            p(x_0,x_1,x_2,x_3) = 
            \begin{cases}
                1 + x_0       & (x_2 = 0, x_3 = 0)\\
                0             & (x_2 = 1, x_3 = 0)\\
                1 + x_0 + x_1 & (x_2 = 0, x_3 = 1)\\
                1 + x_1       & (x_2 = 1, x_3 = 1).
            \end{cases}
        \end{equation*} 
        which we can treat like four independent polynomials!
        
    \end{frame}

\begin{frame}
    \frametitle{References}
    \printbibliography
\end{frame}


\end{document}
Möbius