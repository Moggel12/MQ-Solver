\section{Preliminaries} \label{sec:prereq}
\subsection{The MQ problem} \label{sec:prereq:problem}
For completeness, this subsection defines the problem in focus in this thesis. \textit{Solutions}, in this case, are any assignments to the input variables that evaluate to $0$ on the polynomial(s) in question, i.e. $p(\hat{\mathbf{x}}) = 0$ means that $\hat{\mathbf{x}}$ is a solution to $p$, whereas $p(\hat{\mathbf{x}}') = 1$ is not a solution.
\begin{defn}[The MQ Problem]\label{sec1:def:mq}
    Given a system of $m$ multivariate quadratic polynomials $\mathcal{P} = \{p_{0}, \dots, p_{m-1}\}$, over the ring of polynomials in $n$ variables $\eff[x_0, \dots, x_{n - 1}]$, find values $\Bar{\mathbf{x}} = (\Bar{x}_0, \dots, \Bar{x}_{n - 1})$ that satisfy
    $$
        p_{0}(\mathbf{\Bar{\mathbf{x}}}) =  p_{1}(\Bar{\mathbf{x}}) = \dots = p_{m-1}(\Bar{\mathbf{x}}) = 0 
    $$
    Here, $\eff = \eff_q$ is a finite field of $q$ elements. For this thesis $q = 2$, i.e. the field $\mathbb{F}_2 = \mathbb{Z}_2$.
\end{defn}

\subsection{Bit-slicing, partial evaluation and linearization}
Two common ideas in the space of polynomials over $\eff_2[x_0,\dots x_{n - 1}]$ are those of bit-slicing and partial evaluation. While bit-slicing refers to an optimization technique, partial evaluation is not in itself such. However, partial evaluation of polynomials can be used in tandem with other techniques, for better optimizations.

In this thesis, all \textit{quadratic} polynomials given as input to solvers, sub-procedures, etc. are expected to be \textit{linearized}. This is a rather simple idea, in which a quadratic polynomial is turned into a \textit{multilinear} polynomial. Consider the polynomial
$$
    p(x_0, x_1) = 1 + x_0 + x_0x_1 + x_1^2, 
$$
with two terms of quadratic \textit{total degree}. However, notice that the two terms of total degree two are rather different. The term $x_1^2$ is quadratic in \textit{one} variable, whereas $x_0x_1$ is \textit{linear} in each variable. Since the polynomial is defined over $\eff_2[x_0,x_1]$, the monomial $x_1^2$ acts exactly like the monomial $x_1$. From this fact, $p$ may as well be rewritten as 
\begin{equation} \label{eq:multilinear_p}
    p(x_0, x_1) = 1 + x_0 + x_1 + x_0x_1,
\end{equation}
which is what \textit{linearization} revolves around. Instead of including monomials of degree $2$ in one variable, a procedure may as well preprocess the polynomial(s) making them \textit{multilinear} like \cref{eq:multilinear_p}.

\subsubsection{Partial evaluation} \label{sec:prereq:partial_eval}
Consider the polynomial $p$, defined over $\eff_2[x_0,x_1,x_2,x_3]$,
$$
    p(x_0, x_1, x_2, x_3) = 1 + x_0 + x_2 + x_0x_2 + x_1x_3 + x_2x_3.
$$
This polynomial may be partially evaluated by assigning a subset of its variables a value, thereby obtaining a ''new'' polynomial. Now, assigning (fixing) $k$ variables has $2^k$ possibilities. Say $k = 2$ for $p$ above, one approach is to fix the latter $k$ variables, $x_{n - k - 1} \dots x_{n - 1}$, i.e. for this example $(x_2,x_3)$. This now results in 
\begin{equation}
    p(x_0,x_1,x_2,x_3) = 
    \begin{cases}
        1 + x_0       & (x_2 = 0, x_3 = 0)\\
        0             & (x_2 = 1, x_3 = 0)\\
        1 + x_0 + x_1 & (x_2 = 0, x_3 = 1)\\
        1 + x_1       & (x_2 = 1, x_3 = 1),
    \end{cases}
\end{equation} 
which may then be treated as four separate polynomials:
\begin{equation*}
    \begin{split}
        p^{(0)}(x_0, x_1) &= 1 + x_0          \\
        p^{(1)}(x_0, x_1) &= 0                \\           
        p^{(2)}(x_0, x_1) &= 1 + x_0 + x_1    \\
        p^{(3)}(x_0, x_1) &= 1 + x_1.
    \end{split}
\end{equation*}
Due to how these polynomials were generated, solving one of them will provide part of a solution for the original polynomial $p$. Using this idea, multiple independent polynomials can be manipulated (in parallel), while keeping a strong ''relation'' to the original polynomial. In case a solution for one of the $p^{(i)}$s above was found, simply extending its solution by the assignments of $x_2$ and $x_3$ is enough to \textit{convert} the solution to one for $p$. I.e., $(x_0 = 1, x_1 = 0)$ is a solution for $p^{(0)}$, implying that $(x_0 = 1, x_1 = 0, x_2 = 0, x_3 = 0)$ is a solution for $p$. Assuming sufficient memory, this approach can also be used on entire systems.

\subsubsection{Bit-slicing}
Bit-slicing is another common optimization strategy when working with boolean functions, or in this case polynomials over $\eff_2$. Bit-slicing is a parallelization strategy, which is fundamentally SIMD operations on 1-bit words. The bit-sliced procedure combines multiple bits into a single register in order to use the machine's native bit-wise operations like \texttt{xor}, \texttt{or}, and \texttt{and}, as SIMD instructions on this data. This approach is a cheap method for operating on multiple polynomials (potentially entire systems) at once, e.g. when evaluating some variable assignment on a system. In that case, the operations performed on the individual polynomial terms are the same and so the operations may as well be performed for each polynomial in parallel.

A small example of bit-slicing could be the following polynomials
\begin{equation*}
    \begin{split}
        p_0(x_0, x_1) &= 1 + x_1 + x_0x_1\\
        p_1(x_0, x_1) &= x_0 + x_1,
    \end{split}
\end{equation*} 
that belong to a system of polynomials, $\mathcal{P} = \{p_0, p_1\}$. Choosing to write also the implicit coefficients of the polynomials (assuming the polynomials are \textit{multilinear}),
\begin{equation*}
    \begin{split}
        p_0(x_0, x_1) &= 1 + 0x_0 + 1x_1 + 1x_0x_1\\
        p_1(x_0, x_1) &= 0 + 1x_0 + 1x_1 + 0x_0x_1.
    \end{split}
\end{equation*} 
A procedure could fit either $p_0$ or $p_1$ into a register (byte-sized or more), by assigning each coefficient a bit-position. Assuming the procedure at some point seeks to do the same operation on terms of both $p_0$ and $p_1$ it might as well combine multiple terms into a single register, such that 
$$
    [01_2, 10_2, 11_2, 01_2]
$$
is how the system is represented, \textit{bit-sliced}. Using the bit-wise operands of \texttt{xor} and \texttt{and} alongside the representation above, all polynomials of the system can be manipulated in parallel. E.g. an evaluation of both polynomials, for the assignment $(x_0 = 0, x_1 = 1)$, can be computed by
\begin{equation*}
                01_2 \oplus (00_2 \wedge 10_2) \oplus (11_2 \wedge 11_2) \oplus (00_2 \wedge 11_2 \wedge 01_2) = 10_2,
\end{equation*}
implying the evaluations be $p_0(0,1) = 0$ and $p_1(0,1) = 1$.

The combination of multiple data into one integer, or \textit{register}, does not have to be related to polynomials either. Anytime the data can be represented like this, multiple parallel operations on said data can be done simply with bit-wise operands and integers/registers.

\subsection{Polynomial method for solving MQ systems} \label{sec:prereq:polymethod}
According to \cite{Williams2014ThePM}, the polynomial method was originally a method for proving the limitations of computational devices in circuit complexity. That is, it was originally used for proving limitations on certain types of boolean circuits. Slowly, the method was adapted to algorithm design and has proven a useful tool for this. The general approach for using the polynomial method in algorithm design is to model the computational problem as a boolean circuit, converting it to a corresponding boolean polynomial (exact or probabilistic), to finally manipulate and compute using the polynomial. 

In the field of cryptography, the polynomial method has seen multiple incarnations. Two prominent uses of the method have been for solving $MP$ systems, one being that of \cite{doi:10.1137/1.9781611974782.143} and the other being the main focus of this algorithm, \cite{eurocrypt-2021-30841}. Commonly, the system of polynomials $\mathcal{P} = \{p_i(x_0, \dots x_{n - 1})\}_{i = 0}^{m - 1}$ are represented by 
$$
    F(x_0, \dots x_{n - 1}) = (1 + p_0(x_0, \dots x_{n - 1}))(1 + p_1(x_0, \dots x_{n - 1})) \dots (1 + p_{m - 1}(x_0, \dots x_{n - 1}))
$$
with addition and multiplication being the respective operators of $\eff_2$. Due to the structure of $F$, any solution (common zero) to $\mathcal{P}$ will evaluate to 1 on $P$. For larger systems, and of large degree, $F$ can be hard to efficiently manipulate and is therefore often replaced by a polynomial, $\Tilde{F}$, also called a \textit{probabilistic polynomial}. Ensuring this probabilistic polynomial has a lower degree can help make it efficient the process more efficient, thereby trading determinism for efficiency.

The aforementioned structure using probabilistic polynomials is the basis of some of the core ideas in Dinur's polynomial method solver. The theoretical specification and properties of this algorithm are outlined in \cref{sec:dinur}. For a more in-depth look at the algorithm please refer to the original paper in \cite{eurocrypt-2021-30841}, which also lists cryptanalytic use cases for the algorithm.

\subsection{Fast exhaustive search for multivariate polynomials} \label{sec:prereq:fes}
The fast exhaustive search procedure for polynomials over $\eff_2$ was described and implemented in \cite{ches-2010-23990, cryptoeprint:2013/436, tungchoumasters},
is an important algorithm in the realm of practical MQ-solvers. This algorithm, typically denoted \textit{FES}, is an exhaustive search algorithm for polynomial systems with coefficients in $\eff_2$ in $n$ variables and $m$ polynomials of degree $d$. FES seeks to minimize the operations needed when computing all solutions of a system of multivariate polynomials. The algorithm has proven practical for real-world purposes as it was implemented on multiple occasions, with good use of the parallelization resources present in modern computers. The algorithm computes all common zeroes in maximally $2d\cdot \log_2n \cdot 2^n$ bit operations
for systems of degree-$d$ polynomials and is a core element in Dinur's polynomial-method algorithm from \cite{eurocrypt-2021-30841}.

To give an intuition of the algorithm, consider the polynomial
$$
    p(x_0, x_1, x_2) = 1 + x_0 + x_2 + x_0x_2 + x_1x_2.
$$
A simple way to start an exhaustive search of solutions for $p$ is by evaluating $p(0, 0, 0) = 1$, as this is simply the constant term of $p$. From this point, instead of fully re-evaluating $p$ on a new input, assigning $x_0 = 1$ and keeping the previous assignments of $x_2$ and $x_3$ means that only the terms affected by $x_0$ need to be re-evaluated. Therefore, evaluating the next assignment boils down to 
$$
    p(1, 0, 0) = p(0, 0, 0) + (1 + (1 \cdot 0)) = 1 + (1 + 0) = 0.
$$
Now, observe how the computation above is similar to 
$$
    p(0, 0, 0) + \frac{\partial p}{\partial x_0}(1, 0, 0),
$$
where $\frac{\partial p}{\partial x_0} = 1 + x_2$. Not counting how the derivative itself is obtained, evaluating $p(1, 0, 0)$ with this method is cheaper than fully re-evaluating it term-by-term.

Using the approach just described, alongside an integer counter for enumerating the variable assignments, the subsequent evaluation would be $p(0,1,0)$. Stepping from $p(1,0,0)$ to $p(0,1,0)$ multiple bits changed, hence multiple assignments changed (in this case those for $x_0$ and $x_1$). For larger iteration counts, up to $n$ variables may change assignment between evaluations, hinting at the need for an \textit{alternative} method of enumeration. The ideal case for the derivative approach is to have only \textit{one} variable change between assignments. Enumerating all $2^n$ assignments in such a way can be done through Gray code ordering, i.e. $000_2, 001_2, 011_2, 010_2, 110_2, 111_2, \dots$ and so on.

Finally, earlier it was hinted that computing these derivatives need to be done efficiently. By storing computed evaluations and derivatives each time they are re-computed, future evaluations and derivatives simply needs to do a lookup on the previous value. This way, first-order derivatives may be computed by adding to their previous \textit{evaluation} the appropriate second-order derivative, depending on what variables changed between the current and previous evaluation of this first-order derivative. Of course, these principles can be generalized quite well to degree $d$ polynomials, stopping this \textit{recursive} idea at $d$th-order derivative.

\subsubsection{Gray codes} \label{sec:prereq:fes:gray_codes}

\begin{table}[t]
    \begin{center}
        \begin{tabular}{||c|c|c||}
            \hline
            Index & Binary & Gray \\
            \hline
            0 & $000_2$ & $000_2$\\
            1 & $001_2$ & $001_2$\\
            2 & $010_2$ & $011_2$\\
            3 & $011_2$ & $010_2$\\
            4 & $100_2$ & $110_2$\\
            5 & $101_2$ & $111_2$\\
            6 & $110_2$ & $101_2$\\
            7 & $111_2$ & $100_2$\\
            \hline
        \end{tabular}
    \end{center}
    \caption{3-bit Gray codes and their corresponding binary numerals.} \label{tbl:gray}
\end{table}

An essential part of the innards of FES is that of \textit{Gray codes} or \textit{reflected binary codes}. This is fundamentally an ordering of the binary numbers. In this ordering, any two consecutive numbers will differ in only \textit{one} bit. An example of this is the binary numeral of the decimal 3, using three bits, being $011_2$ whereas its corresponding Gray code is $010_2$.
The consecutive value, decimal 4, has the binary numeral $100_2$ and Gray code $110_2$. These codes have various properties making them applicable in error correction, position encoders, and more. The type of Gray code used in the FES procedure from \cite{ches-2010-23990, cryptoeprint:2013/436,tungchoumasters} is not the only one, as multiple other types with different additional properties exist. For an example of the Gray code ordering used in FES see \cref{tbl:gray}.

To construct the sequence of all $n$-bit binary reflected Gray codes, a recursive formulation can be used. The idea is, in each recursive step, to reflect$\rightarrow$append$\rightarrow$prepend. Starting with the sequence $0_2, 1_2$, one first \textit{reflects} the sequence (resulting in $1_2, 0_2$) and \textit{appends} it to the original sequence. Lastly, one prepends $0$ to entries of the first half of this new sequence and $1$ on the latter half. The sequence is now $00_2, 01_2, 11_2, 10_2$. These steps can then be repeated until the codes are formed of $n$ bits.

Using the approach above, a ''closed formula'' for computing the $i$th codeword can be derived. For the sake of brevity, the connection will not be described more than by noting its existence. The formula for computing the $i$th codeword is
$$
    g_i = i \oplus (i \rightshift 1)
$$
where $\rightshift$ denotes a logical right-shift operation in the binary representation of $i$ and $\oplus$ denotes the bit-wise \textit{xor} operation between two binary numbers.

\subsubsection{Exhaustive Search using Gray Codes} \label{sec:prereq:fes:exh_g_code}
\begin{figure}[t]
    \begin{alg}
        \caption{EVAL($p$, $n$)}\label{alg:fes_eval}
        \KwIn{$p$, $n$}
        \KwResult{List of common zeroes of $p$}
        $state \gets \text{INIT($p$, $n$)}$\;
        \If{$state.y = 0$}{
            Add $state.y$  to list of common zeroes\;
        }
        \While{$state.i < 2^n$}{
            STEP($state$)\;
            \If{$state.y = 0$}{
                Add $state.y$  to list of common zeroes\;
            }
        }
        \Return List of common zeroes
    \end{alg}
    \caption{Pseudo-code for the total evaluation procedure of FES (quadratic polynomials)}
\end{figure}

\begin{defn}[] \label{sec:prereq:def:bk}
    Let $b_\alpha(i)$ denote the $k$th least significant bit in the binary representation of the decimal $i$. If $i$ has hamming weight less than $k$, $b_\alpha(i) = -1$.
\end{defn}

\begin{defn}[Derivatives] \label{sec:prereq:deriv}
    Let $\{\mathbf{e}_0, \dots, \mathbf{e}_{n-1}\}$ denote the canonical basis over the vector-space $(\eff_2)^n$. The derivative of a polynomial, $p$, in the ring $\eff_2[x_0,\dots,x_{n-1}]$ w.r.t. the $j$th variable is $\frac{\partial p}{\partial x_j} : \mathbf{x} \mapsto p(\mathbf{x} + \mathbf{e}_j) + p(\mathbf{x})$. This is also called the \textit{finite difference} in the $x_j$ dimension of the multivariate polynomial $p$. Recall that the $+$ operator above is over $\eff_2$, acting like an exclusive-or operation.
\end{defn} 

To minimize the number of operations needed between iterations in an exhaustive search procedure, the authors of \cite{ches-2010-23990} suggest looking at inputs in Gray code order.
Examining inputs in Gray code order allows for efficient use of partial derivatives for computing the evaluation of one input based on the evaluation of the previous input. Inspecting \cref{sec:prereq:deriv} reveals the foundation of this idea. In each iteration of the exhaustive search procedure $p(\mathbf{x}_i)$ needs to be evaluated. Using the Gray code approach, only one variable in the input changes so $\mathbf{x}_i$ and $\mathbf{x}_{i - 1}$ differ in only the $j$th variable, meaning $p(\mathbf{x}_{i - 1} + \mathbf{e}_j) = p(\mathbf{x}_i)$. From \cref{sec:prereq:deriv}, $$
    p(\mathbf{x} + \mathbf{e}_j) = p(\mathbf{x}) + \frac{\partial p}{\partial x_j}(\mathbf{x})
$$ 
which implies that the difference between two evaluations in Gray code order is $\frac{\partial p}{\partial x_j}(\mathbf{x}_{i - 1}) = \frac{\partial p}{\partial x_j}(\mathbf{x}_i)$, and was proven in \cite{tungchoumasters}. Therefore, storing $p(\mathbf{x}_{i - 1})$ and adding $\frac{\partial p}{\partial x_j}(\mathbf{x}_{i-1})$ is sufficient for computing the next evaluation in a Gray-code ordered input sequence.

In other terms, let $i = 0,\dots 2^n-1$ denote the iteration count for the FES procedure or the current index into the Gray code sequence of $n$-bit codewords. Between two consecutive steps of FES, say $i = 10$ and $i = 11$, the Gray codes $g_{10} = 1111_2$ and $g_{11} = 1110_2$ differ in only the least significant bit. Letting $\mathbf{x}_{10}$ and $\mathbf{x}_{11}$ be vector forms of $g_{10}$ and $g_{11}$, respectively, the difference between $p(\mathbf{x}_{10})$ and $p(\mathbf{x}_{11})$ is exactly $\frac{\partial p}{\partial x_0}(\mathbf{x}_{11})$. In this example, since $x_0$ was the only variable that changed, the partial derivative w.r.t. $x_0$ represents the only parts of $p$ that change between evaluations of $\mathbf{x}_{10}$ and $\mathbf{x}_{11}$.

Now, using the idea of derivatives will reduce the evaluation of degree $d$ polynomials to that of evaluating a degree $d-1$ polynomial (i.e. a first-order partial derivative). However, stopping here leaves what \cite{ches-2010-23990} denotes as the \textit{folklore differential technique}. Consequently, the original authors devised the FES algorithm by (amongst other things) recursively applying this derivative idea. This means that between $i = 10$ and $i = 11$, the algorithm stores the latest $\frac{\partial p}{\partial x_j}(\mathbf{x})$ that has been computed and ensures to update it by recursively looking at the only variable, $x_l$, that changed since last time $x_j$ toggled. This means that $\frac{\partial p}{\partial x_j}(\mathbf{x})$ may be computed by adding $\frac{\partial^2 p}{\partial x_j \partial x_l}(\mathbf{x})$ to the previous evaluation of $\frac{\partial p}{\partial x_j}$. For quadratic polynomials, this second derivative would be a constant (stored in a lookup table) whereas running FES on systems of degree $d$ means recursing $d$ times. 

\begin{figure}[t]
    \begin{alg}
        \caption{INIT($p$, $n$)}\label{alg:fes_init}
        \KwIn{$p$, $n$}
        State $state$\;
        $state.i \gets 0$\;
        $state.y \gets p.\text{constant\_coefficient()}$\;
        \ForEach{$k = 1,\dots n-1$}{
            \ForEach{$j = 0,\dots k - 1$}{
                $s.d''[k,j] \gets p.\text{monomial\_coefficient}(k, j)$\;
            }
        }
        $s.d'[0] \gets p.\text{monomial\_coefficient(0)}$\;
        \ForEach{$\alpha = 1,\dots n-1$}{
            $s.d'[\alpha] \gets s.d''[\alpha, \alpha - 1] \oplus p.\text{monomial\_coefficient}(\alpha)$\;
        }
        \Return $state$\;
    \end{alg}
    \caption{Pseudo-code for the initialization procedure of FES (quadratic polynomials)}
\end{figure}

Letting $\mathbf{v}_i$ be the vector-form of the counter $i$ (non-Gray code), the FES procedure looks not only for $b_1(\mathbf{v}_i)$ (\cref{sec:prereq:def:bk}) but $b_\alpha(\mathbf{v}_i)$ for $\alpha=1,\dots,d$. For the quadratic case in \cref{alg:fes_step} the procedures BIT$_1$ and BIT$_2$ represent $b_1(\mathbf{v}_i)$ and $b_2(\mathbf{v}_i)$ with \texttt{state.i} representing $\mathbf{v}_i$. The fact that the bits that are changed can be found through the (non-Gray code) counter can be derived through the construction of $n$-bit sequences of binary reflected Gray codes, or see the proof in \cite{tungchoumasters}. The pseudo-code for the previously described process resides in \cref{alg:fes_step}, showing both the storage of first and second derivatives as well as the computation of \texttt{state.y} which corresponds to $p(\mathbf{x}_i)$. Clearly, this idea shows how storage is one of the weaknesses of FES, especially for polynomials of degree $d > 2$. 

Due to this structure, FES is required to have initialized these derivative values for whenever it encounters the \textit{first toggle} of each bit, i.e. when \texttt{state.i + 1} sets a bit in a position that so far has been untouched. Therefore, some time is spent pre-evaluating these derivative values directly. For the quadratic case, the pre-evaluation is done in \cref{alg:fes_init}, lines 10-13, equivalently the value stored is $\frac{\partial p}{\partial x_\alpha \partial x_{\alpha-1}} + a_\alpha$ for $a_\alpha$ being the coefficient to the monomial $x_\alpha$. An example where this is necessary is when evaluating $p(1,1,0,0)$, at which point no previous $\frac{\partial p}{\partial x_1}$ evaluations exist (if not initialized). This derivative would need to be initialized to $\frac{\partial p}{\partial x_1}(\mathbf{e}_0) = \frac{\partial p}{\partial x_1 \partial x_0}$, as is shown in \cite{tungchoumasters}. This last example assumes $p$ is a quadratic polynomial.

To see that the recursive procedure above may be further optimized, one may observe that running such a procedure on a single polynomial, $p$, of degree $d$ yields a complexity of $O(d\cdot2^n)$ and consuming $O(n^d)$ bits of memory. These facts were proven in \cite{ches-2010-23990}. This paper further introduces smaller optimizations to the theoretical construction of the algorithm, such as removing computations of a \texttt{state.x} variable, which in \cref{alg:fes_eval}, \ref{alg:fes_init} and \ref{alg:fes_step} have already been applied. 

As is also stated in \cite{ches-2010-23990}, this initial FES version may be parallelized quite nicely, due to the evaluation procedure doing computations independently from the coefficients of the polynomials. This means that running an instance per $p_i \in \mathcal{P} = \{p_0, \dots p_{m-1}\}$ is possible, where each instance is essentially running on independent data. Extending the values (e.g. \texttt{state.y}) of \cref{alg:fes_eval}, \ref{alg:fes_init}, and \ref{alg:fes_step}, to be bit-vectors instead of a singular bit then yields a bit-sliced version that may use bit-wise instructions of \texttt{xor} and \texttt{and}, potentially computing a full system in one pass (in parallel). For \cref{alg:fes_init}, \cref{alg:fes_eval} and \cref{alg:fes_step}, the pseudo-code only represents FES on single polynomials, but it should still be clear how to parallelize using bit-slicing and inputting entire systems $\mathcal{P}$ (with appropriate method calls) instead of single polynomials $p$. 

\begin{figure}[t]
    \begin{alg}
        \caption{STEP($state$)} \label{alg:fes_step}
        \KwIn{$state$}
        $state.i \gets state.i + 1$\;
        $\alpha_1 \gets \text{BIT}_1(state.i)$\;
        $\alpha_2 \gets \text{BIT}_2(state.i)$\;
        \If{$\alpha_2$ exists in $state.i$}{
            $s.d'[\alpha_1] \gets s.d'[\alpha_1] \oplus s.d''[\alpha_1,\alpha_2]$\;
        }
        $s.y \gets s.y \oplus s.d'[\alpha_1]$\;
    \end{alg}
    \caption{Pseudo-code for the procedure handling derivatives and evaluations in FES (quadratic polynomials).}
\end{figure}

\subsubsection{Partial evaluation and FES}

Two important ideas for obtaining a sufficient parallelization of the combined procedure (in practice) from \cref{sec:prereq:fes:exh_g_code}; using bit-vectors for collecting bit operations of multiple polynomials in the system as well as \textit{partial evaluation}. By fixing $k$ variables, say $x_{n-k}$ to $x_{n-1}$, $2^k$ new systems may be obtained in which each $p_i \in \mathcal{P}$ is partially evaluated on some corresponding permutation of $k$ bits. Partial evaluation is explained in more detail in \cref{sec:prereq:partial_eval}.

The approach used by the authors of \cite{ches-2010-23990, cryptoeprint:2013/436} was therefore to select $w$ polynomials, fixing $k$ variables and producing $2^k$ smaller systems of $w$ polynomials which are searched using the recursive procedure described in \cref{sec:prereq:fes:exh_g_code}. Any common zero of these systems is then checked against the remaining $m-w$ polynomials. This approach achieves the proclaimed complexity of $2d\cdot\log_2 n \cdot 2^n$, as proved in \cite{ches-2010-23990}, which consequently is the one Dinur requires for his polynomial-method algorithm of \cite{eurocrypt-2021-30841}.

\subsubsection{Generalized FES} \label{sec:ext:fes_interp:g_fes}
The ideas behind a generalized FES approach are mostly similar to what has already been described. The idea is still to use derivatives in order to minimize the operations needed to compute new evaluations of the polynomial(s). However, since input polynomials may now be of degree $d > 2$, the procedure has to use the appropriate high-order derivatives.

Consider the polynomial
$$
    p(x_0, x_1, x_2) = 1 + x_0 + x_0 x_1 + x_0 x_1 x_2
$$
using the FES approach detailed in \cref{sec:prereq:fes}, it is clear that 
$$
    \frac{\partial^2 p}{\partial x_0 \partial x_1} = 1 + x_2.
$$
Second-order partial derivatives are no longer constants, instead the procedure must use third-order partial derivatives for polynomials like $p$ above, or $d$-order partial derivatives for degree $d$ polynomials in general. Therefore, the derivatives must now be stored in more and/or larger tables, as the procedure has to store up to $d$-order partial derivatives. These $d$-order partial derivatives act like constants, just like second-order partial derivatives did for quadratic polynomials. All other partial derivatives of order $< d$ are to be computed similarly to the first-order partial derivatives of \cref{alg:fes_step}. 

Consider \cref{alg:generalized_fes}; the algorithm is an extension of \cref{alg:fes_eval}, that now computes the zeros of any polynomial of degree $d$, using the ideas discussed above. The dictionary $D$ stores the derivative values of monomials just like $s.d'$ and $s.d''$ did in \cref{alg:fes_step} and \cref{alg:fes_init}. However, in this approach the algorithm stores all derivatives in a single dictionary. For the sake of simplicity, any lookup on an entry in $D$ that has not been initialized is defaulted to return $0$. Further, monomials can be represented as lists of the indices of 1-bits in an $n$-length bitstring, representing which of the $n$ variables are \textit{active} in the monomial. An example could be the monomial $x_0x_2x_3$ that may be represented by the bitstring (or bit-vector) $1101_2$. The key zero, $D[0]$, corresponds to $s.y$ in \cref{alg:fes_eval}.

The \texttt{DERIVATIVE\_INIT()} procedure in the pseudo-code initializes the derivative table entries to their initial values. For the quadratic case (\cref{alg:fes_init}), this was a rather simple task of initializing the second-order derivative table simply as appropriate coefficients, followed by initializing the first-order derivative table to 
$$
    \frac{\partial p}{\partial x_\alpha}(\mathbf{x}_{2^\alpha}) = \frac{\partial^2 p}{\partial x_{\alpha} \partial x_{\alpha - 1}} + c_\alpha
$$
where $c_\alpha$ is the coefficient to the monomial $x_\alpha$. Notice, the notation from \cref{sec:prereq:fes:exh_g_code}, $\mathbf{x}_{2^\alpha}$, is used to denote the Gray code value of $2^\alpha$ as a vector in $\eff_2^n$. In the general case, the goal is the same, however, for a potentially larger degree. This means that the general approach is to initialize entries to 
$$
    \frac{\partial^j p}{\partial x_{\alpha_1} \dots \partial x_{\alpha_j}}(\mathbf{x}_{2^{\alpha_1} + \dots + 2^{\alpha_j}})
$$
with $\alpha_1 \dots, \alpha_j$ being the contents of the bit-vector representation of monomials described earlier. Again, formal proofs for why these initializations are used can be found in \cite{tungchoumasters}.

The early parts of the for-loop at \cref{alg:generalized_fes:step} is the generalized version of the stepping procedure, \cref{alg:fes_step}. Instead of only computing the positions of the two least significant one bits, for counter value $i$, the procedure instead computes the positions of the least $d$ 1-bits. The amount of bit positions stored depends on whether or not the counter value $i$ has $\leq d$ bits, if more the procedure only chooses the first $d$ bits. The $Depth$ variable in the pseudo-code represents the number of bits chosen, or how \textit{deep} the ''recursion'' should go. Inspecting \cref{alg:generalized_fes:step:compute}, it should be clear that new derivative values are computed in a recursive manner, where low-order derivatives are computed by adding the high-order derivatives, i.e.
\begin{equation} \label{eq:fes_recurse}
    \frac{\partial^{j - 1} p}{\partial x_{\alpha_0} \dots \partial x_{\alpha_{j - 2}}}(\mathbf{x}_i) = Q_{x_{\alpha_0} \dots x_{\alpha_{j - 1}}} + \frac{\partial^j p}{\partial x_{\alpha_0} \dots \partial x_{\alpha_{j - 1}}}(\mathbf{x}_i).
\end{equation}
Here, $Q_{x_{\alpha_0} \dots x_{\alpha_{j - 1}}}$ is the previous evaluation of $\frac{\partial^{j - 1} p}{\partial x_{\alpha_0} \dots \partial x_{\alpha_{j - 2}}}$, i.e. the entry stored in the derivative table. It should be noted here that recursively applying the equation above will resolve when reaching the base case of $\partial^0 p = p$, being the polynomial $p$ itself.

Now, since the procedure stores the evaluation of $p(\mathbf{x}_i)$ in $D[0]$, the only missing part is to check if the evaluation is a common zero. This takes places at \cref{alg:alg:generalized_fes:check} and onwards.

\begin{figure}[ht]
    \begin{alg}
        \KwIn{A polynomial $p$ alongside its number of variables $n$ and degree $d$.}
        \KwResult{All zeros of the polynomial $p$.}
        $Solutions \gets []$\;
        $k \gets 0$\;
        $D \gets \text{DICT(default: 0)}$\;
        $D[0] \gets p.\text{constant\_coefficient()}$\;
        \ForEach{$Mon$ in $p$.monomials()}{
            $D[Mon.\text{bits()}] \gets \text{DERIVATIVE\_INIT}(Mon)$\; \label{alg:generalized_fes:init}
        }
        \ForEach{$i = 0, \dots 2^n - 1$}{ \label{alg:generalized_fes:step}
            $Depth \gets \min(\text{HAMMING\_WEIGHT}(i), d)$\;
            $\alpha \gets \text{BITS}(i, Depth)$\;
            \ForEach{$j = Depth \dots, 1$}{
                $D[\alpha_{0\dots j - 1}] \gets D[\alpha_{0\dots j - 1}]] \oplus D[\alpha_{0\dots j}]$\; \label{alg:generalized_fes:step:compute}
            }
            \If{$D[0] = 0$}{ \label{alg:alg:generalized_fes:check}
                $Solutions[k] \gets \text{GRAY}(i)$\;
                $k\pp$\;
            }
        }
        \Return{Solutions}
        \caption{GENERALIZED\_FES($p$, $n$, $d$)} \label{alg:generalized_fes}
    \end{alg}
    \caption{A generalized FES procedure for degree $d$ polynomials.}
\end{figure}

\subsection{Polynomial interpolation} \label{sec:prereq:poly_interp}
Polynomial interpolation is quite an important concept in terms of Dinur's polynomial-method algorithm. The Möbius transform, as described in \cite{joux2009algorithmic}, enables obtaining the algebraic normal form (ANF) of a boolean function using its truth table as input. The algebraic normal form of a boolean function $p$ on $n$ variables is written
$$
    p(x_0, \dots x_{n - 1}) = \bigoplus_{(a_0,\dots a_{n - 1}) \in \eff_2^n} g(a_0, \dots a_{n - 1}) \prod_i x_i^{a_i}
$$
where $g$ is the Möbius transform. The Möbius transform $g$ is a boolean function itself and may therefore be implemented using bit operations. The ANF of $p$ may be represented with a bit vector of length $2^n$. A closer inspection of the relationship between $g$ and $p$ further reveals that the Möbius transform can be computed using the formula
$$
    p(x_0, \dots x_{n - 1}) = p^{(0)}(x_0, \dots x_{n - 2}) \oplus p^{(1)}(x_0, \dots x_{n - 2}) \cdot x_{n - 1}
$$
where $p^{(0)}$ and $p^{(1)}$ are defined as 
\begin{equation*}
    \begin{split}
        p^{(0)}(x_0, \dots x_{n - 2}) &= p(x_0, \dots x_{n - 2}, 0), \\
        p^{(1)}(x_0, \dots x_{n - 2}) &= p(x_0, \dots x_{n - 2}, 0) \oplus p(x_0, \dots x_{n - 2}, 1).
    \end{split}
\end{equation*}
Now, a nice property of the Möbius transform is that it is an involution, and so it computes both the ANF of a boolean function/polynomial $p$ via its truth table but may also use the polynomial as input and obtain its truth table, i.e. a full evaluation on its input-space.

\begin{figure}[t]
    \begin{alg}
        \caption{MOB\_TRANSFORM($S$, $n$)}
        \KwIn{The truth table $S$ of the boolean function $p$ on $n$ variables, with $2^n$ entries.}
        \KwResult{The Möbius transform of $p$, contained in the original list of $S$.}
        \ForEach{$i = 0, \dots n - 1$}{
            $Sz \gets 2^i$\;
            $Pos \gets 0$\;
            \While{$Pos < 2^n$}{
                \ForEach{$j = 0, \dots Sz - 1$}{
                    $S[Pos + Sz + j] \gets S[Pos + j] \oplus S[Pos + Sz + j]$\;
                }
                $Pos \gets Pos + 2 \cdot Sz$\;
            }
        }
    \end{alg}
    \caption{Pseudo-code of an implementation of the Möbius transform.}
    \label{alg:mob}
\end{figure}

There are different ways of computing the Möbius transform in practice. One method is the \textit{in-place} implementation, in which the input truth table is overwritten in place. This variant is described \cref{alg:mob} and an example implementation may be found in \cite{joux2009algorithmic}, alongside a more detailed description of the Möbius transform and related transforms. It should also be noted that the algorithm in \cref{alg:mob} is consistent with the involution property of the Möbius transform.

Finally, using the Möbius transform for fully evaluating a polynomial may be a viable option, comparing it against naively evaluating the polynomial on all inputs one by one. The works of \cite{fse-2011-23547} show how one may approach such use of the Möbius transform, using the constructs above, and argues of its complexity.

\subsubsection{Transformations on sparse data} \label{sec:prereq:poly_interp:sparse}
When interpolating polynomials, not all $2^n$ assignments of the $n$ variables are needed. In \cite{eurocrypt-2021-30841} it was noted that the Möbius transform algorithm may be adapted to work with 
$$
    \sum_{i = 0}^{d} \binom{n}{i}
$$
evaluations in the input array, for a degree $d \leq n$ polynomial of $n$ variables. For this to work, only evaluations of low hamming weight input vectors ($\leq d$) are used, instead of a full evaluation. It was also argued in \cite{eurocrypt-2021-30841} that such an adaptation would yield a time complexity of $\leq n \cdot \sum_{i = 0}^{d}\binom{n}{i}$ bit operations.

\newpage
