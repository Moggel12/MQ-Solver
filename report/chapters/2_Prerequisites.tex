\section{Prerequisites (approx. 10-15 pages)} \label{sec:prereq}

\subsection{Polynomial method for solving MQ systems} \label{sec:prereq:polymethod}

\subsection{Fast exhaustive search for multivariate polynomials}
\label{sec:prereq:fes}
\td{ADD DESCRIPTION OF PREPROCESSING POLYNOMIALS}
\td{ADD EXAMPLE SECTION}
The fast exhaustive search procedure for polynomials over $\eff_2$, \cite{cryptoeprint:2010/313, cryptoeprint:2013/436},
\comment{Commas around citations are strange here...}
is an important algorithm in the realm of practical MQ-solvers. This algorithm, typically denoted \textit{FES}, is an exhaustive search algorithm for polynomial systems with coefficients in $\eff_2$ in $n$ variables and $m$ polynomials of degree $d$. FES seeks to minimize the operations needed when computing all solutions of a system of multivariate polynomials. The algorithm can be implemented nicely
\comment{What does 'nicely' mean?}
in practice, with good use of the parallelization resources present in modern computers. The algorithm needs $2d\cdot \log_2n \cdot 2^n$ bit operations (in expectation)
\comment{I believe that is the maximum... Check!}
for systems of quadratic polynomials, and is a core element in Dinur's polynomial-method algorithm from \cite{cryptoeprint:2021/578}.

\comment{I would recomment to provide an intuition here (see my mail on that)
and to move and reference the algorithms, say, at the end of Section 2.2.2.}

The main FES procedure is shown in \cref{alg:fes_eval} with the helper functions \cref{alg:fes_init} and \cref{alg:fes_step}. This merely serves to give context to the following subsections.

\subsubsection{Gray codes} \label{sec:prereq:fes:gray_codes}
\td{Revisit}

An essential part of the innards of FES is that of \textit{gray codes} or \textit{reflected binary codes}. This is fundamentally an ordering of the binary numbers. In this ordering, any two consecutive numbers will differ in only \textit{one} bit. An example of this is the binary encoding of the decimal 3, using three bits, being $011_2$ whereas its corresponding gray code is $010_2$.
\comment{So --- is it an ordering of numbers or an encoding of numbers? You can
see it as either --- but don't mix!}
The consecutive value, decimal 4, has the binary encoding $100_2$ and gray code $110_2$. These codes have various properties making them applicable in error correction, position encoders and more. The type of gray code used in the FES procedure from \cite{cryptoeprint:2010/313, cryptoeprint:2013/436} is not the only one, as multiple other types with different additional properties exist.

To construct the sequence of all $n$-bit binary reflected gray codes, a recursive formulation can be used. The idea is, in each recursive step, to reflect-append-prepend. Starting with the sequence $0_2, 1_2$, one first \textit{reflects} the sequence (being $1_2, 0_2$) and \textit{appends} it to the original sequence. Lastly, one prepends $0$ to entries of the first half of this new sequence and a $1$ on the latter half. The sequence is now $00_2, 01_2, 11_2, 10_2$. These steps can then be repeated until the codes are formed of $n$ bits.

From the construction method just explained, it can be derived that constructing the $i$th codeword, $g_i$ can be done using the formula
$$
    g_i = i \oplus (i \rightshift 1)
$$
\comment{The connection between the reflect-append-prepend construction and the
formula is unclear from this explanation.}
where $\rightshift$ denotes a logical right-shift operation in the binary representation of $i$ and $\oplus$ denotes the bit-wise \textit{xor} operation between two binary numbers. This formula can be derived by observing that inverting the bit at position $i$ of all binary encoded numbers in the sequence $0,\dots 2^{n} - 1$ will change the order of blocks of $2^i$ codewords.

\subsubsection{Exhaustive Search using Gray Codes} \label{sec:prereq:fes:exh_g_code}

\begin{figure}
    \begin{minipage}[t]{.5\linewidth}
        \begin{alg}
            \caption{STEP($state$)}\label{alg:fes_step}
            \KwIn{$state$}
            $state.i \gets state.i + 1$\;
            $k1 \gets \text{BIT}_1\text{(state.i)}$\;
            $k2 \gets \text{BIT}_2\text{(state.i)}$\;
            \If{k2 exists in state.i}{
                $s.d'[k_1] \gets s.d'[k1] \oplus s.d''[k1,k2]$\;
            }
            $s.y \gets s.y \oplus s.d'[k1]$\;
        \end{alg}
        \caption{Step}
        \label{sec2:alg:fes_step}
    \end{minipage}
    \begin{minipage}[t]{.5\linewidth}
        \begin{alg}
            \caption{EVAL($p$, $n$)}\label{alg:fes_eval}
            \KwIn{$p$, $n$}
            \KwResult{List of common zeroes of $p$}
            $state \gets \text{INIT($p$, $n$)}$\;
            \If{$state.y = 0$}{
                Add $state.y$  to list of common zeroes\;
            }
            \While{$state.i < 2^n$}{
                STEP($state$)\;
                \If{$state.y = 0$}{
                    Add $state.y$  to list of common zeroes\;
                }
            }
            \Return List of common zeroes
        \end{alg}
        \caption{Eval}
        \label{sec2:alg:fes_eval}
    \end{minipage}
    \begin{minipage}[H]{\linewidth}
        \begin{alg}
            \caption{INIT($p$, $n$)}\label{alg:fes_init}
            \KwIn{$p$, $n$}
            State $state$\;
            $state.i \gets 0$\;
            $state.y \gets p.\text{constant\_coefficient()}$\;
            \ForEach{$k = 1,\dots n-1$}{
                \ForEach{j = 0,\dots k - 1}{
                    $s.d''[k,j] \gets p.\text{monomial\_coefficient(k, j)}$\;
                }
            }
            $s.d'[0] \gets p.\text{monomial\_coefficient(0)}$\;
            \ForEach{k = 1,\dots n-1}{
                $s.d'[k] \gets s.d''[k, k - 1] \oplus p.\text{monomial\_coefficient(k)}$\;
            }
            \Return $state$\;
        \end{alg}
        \label{sec2:alg:fes_init}
    \end{minipage}
    \caption{INIT}
    \label{fig:fes_subparts}
\end{figure}

\begin{defn}[] \label{sec2:def:bk}
    Let $b_k(i)$ denote the $k$th lowest significant bit
\comment{``least significant bit''?}
in the binary representation of the decimal $i$. If $i$ has hamming weight less than $k$, $b_k(i) = -1$.
\end{defn}

\begin{defn}[Derivatives] \label{sec2:def:deriv}
    Let $\{\mathbf{e}_0, \dots, \mathbf{e}_{n-1}\}$ denote the canonical basis over the vector-space $(\eff_2)^n$. The derivative of a polynomial, $p$, in the ring $\eff_2[x_0,\dots,x_{n-1}]$ w.r.t. the $i$th variable is $\frac{\partial p}{\partial x_i} : \mathbf{x} \mapsto p(\mathbf{x} + \mathbf{e}_i) + p(\mathbf{x})$.
\comment{Explain what this means.}
\end{defn} 

In order to minimize the amount of operations needed between iterations in an exhaustive search procedure, the authors of \cite{cryptoeprint:2010/313} suggests to look at inputs in gray code order.
\comment{``gray code'' $\rightarrow$ ``Gray code'' (several times)}
Examining inputs in gray code order allows for efficient use of partial derivatives for computing the output of one input based on the evaluation of the previous input. Inspecting \cref{sec2:def:deriv} reveals the foundation for this idea. In each iteration of the exhaustive search procedure $p(\mathbf{x}_i)$ needs to be evaluated. Using the gray code approach, only one variable in the input changes so $\mathbf{x}_i$ and $\mathbf{x}_{i - 1}$ differ in only the $j$th variable, meaning $p(\mathbf{x}_{i - 1} + \mathbf{e}_j) = p(\mathbf{x}_i)$. From \cref{sec2:def:deriv}, $$
    p(\mathbf{x} + \mathbf{e}_j) = p(\mathbf{x}) + \frac{\partial p}{\partial x_j}(\mathbf{x})
$$ 
which implies that the difference between two consecutive inputs in gray code order is $\frac{\partial p}{\partial x_j}(\mathbf{x}_{i - 1}) = \frac{\partial p}{\partial x_j}(\mathbf{x}_i)$, and was proven in \cite{tungchoumasters}. Therefore, storing $p(\mathbf{x}_i)$ adding $\frac{\partial p}{\partial x_j}(\mathbf{x}_{i-1})$ is sufficient for computing the next evaluation in a gray-code ordered input sequence.

In other terms, let $i = 0,\dots 2^n-1$ denote the iteration count of the FES procedure, or alternatively the current index into the gray code sequence of $n$-bit codewords. Between two consecutive steps of FES, say $i = 10$ and $i = 11$, the gray codes $g_{10} = 1111_2$ and $g_{11} = 1110_2$ differ in only the least significant bit. Letting $\mathbf{x}_{10}$ and $\mathbf{x}_{11}$ be vector forms of $g_{10}$ and $g_{11}$, respectively, the difference between $p(\mathbf{x}_{10})$ and $p(\mathbf{x}_{11})$ is exactly $\frac{\partial p}{\partial x_0}(\mathbf{x}_{11})$. In the previous example, since $x_0$ was the only variable that changed, the partial derivative w.r.t. $x_0$ represents the only parts of $p$ that change between evaluations of $\mathbf{x}_{10}$ and $\mathbf{x}_{11}$.

Now, using the idea of derivatives will reduce the evaluation of degree $d$ polynomials to that of evaluating a degree $d-1$ polynomial. However, stopping here leaves us with what \cite{cryptoeprint:2010/313} denotes as the \textit{folklore differential technique}. Consequently, the original authors devised the FES algorithm by (amongst other things) recursively applying this derivative idea. This means that between $s = 10$ and $s = 11$, the algorithm stores the latest $\frac{\partial p}{\partial x_i}(\mathbf{x})$ that has been computed and ensures to update it by recursively looking at the only variable, $x_j$, that changed since last time $x_i$ toggled. This means that we may update $\frac{\partial f}{\partial x_i}(\mathbf{x})$ by adding $\frac{\partial^2 p}{\partial x_i \partial x_j}(\mathbf{x})$ to the stored value. For quadratic polynomials, this second derivative would be a constant (stored in a lookup table) whereas running FES on systems of degree $d$ means recursing $d$ times. 

Letting $\mathbf{v}_i$ be the vector-form of the binary encoding of $\mathbf{x}_i$ we have that the FES procedure looks not only for $b_1(\mathbf{v}_i)$ (\cref{sec2:def:bk}) but $b_k(\mathbf{v}_i)$ for $k=1,\dots,d$. For the quadratic case in \cref{sec2:alg:fes_step} we see the procedures BIT$_1$ and BIT$_2$ representing $b_1(\mathbf{x}_i)$ and $b_2(\mathbf{x}_i)$ with \texttt{state.i} representing $\mathbf{v}_i$. The fact that we can simply use the binary encoding to find which bits are turned on and off can be derived through the construction of $n$-bit sequences of binary reflected gray codes, or see the proof in \cite{tungchoumasters}. The pseudo-code for the previously described process reside in \cref{alg:fes_step}, showing both the storage of first and second derivatives as well as the computation of \texttt{state.y} which corresponds to $p(\mathbf{x}_i)$. Clearly, this idea shows how storage is one of the weaknesses of FES, especially for polynomials of degree $d > 2$. 

Due to this structure, FES is required to have initialized these derivative values for whenever it encounters the \textit{first toggle} of each bit, i.e. when \texttt{state.i + 1} sets a bit in a position that so far has been untouched. Therefore, some time is spent for pre-evaluating these derivative values directly. For the quadratic case, the pre-evaluation is done in \cref{alg:fes_init}, lines 10-13, equivalently the valued stored is $\frac{\partial p}{\partial x_k \partial x_{k-1}} + a_k$ for $a_k$ being the coefficient to the monomial $x_k$. An example where this is necessary is when evaluating $p(1,1,0,0)$, at which point no previous $\frac{\partial p}{\partial x_1}$ evaluations exist (if not initialized). This derivative would need to be initialized to $\frac{\partial p}{\partial x_1}(\mathbf{e}_0) = \frac{\partial p}{\partial x_1 \partial x_0}$, as is shown in \cite{tungchoumasters}. This last example assumes $p$ is a quadratic polynomial.

To see that the recursive procedure above may be further optimized, one may observe that running such a procedure on a single polynomial, $p$, of degree $d$ yields a complexity of $O(d\cdot2^n)$ and consuming $O(n^d)$ bits of memory. These facts were proven in \cite{cryptoeprint:2010/313}. This paper further introduces smaller optimizations to the theoretical construction of the algorithm, such as removing computations of a \texttt{state.x} variable, which in \cref{fig:fes_subparts} have already been applied. As is also stated in \cite{cryptoeprint:2010/313}, this initial FES version may be parallelized quite nicely, due to the evaluation procedure doing computations independently from the coefficients of $p$.
\comment{This part is a bit unclear...}
\comment{Maybe introduce bitslicing as a principle..?}
This means that running an instance per $p_i \in \mathcal{P} = \{p_0, \dots p_{m-1}\}$ is possible, where each instance is essentially running on independent data. Extending the values (e.g. \texttt{state.y}) of \cref{fig:fes_subparts} to be bit-vectors instead of a singular bit then yields a version of the recursive procedure that can find all common zeroes of a system in $O(2m2^n)$ bit operations. For \cref{alg:fes_init}, \cref{alg:fes_eval} and \cref{alg:fes_step}, the pseudo-code only represents FES on single polynomials, but should still be clear how to parallelize using bit-vectors and inputting entire systems $\mathcal{P}$ (with appropriate method calls) instead of single polynomials $p$. 

\subsubsection{Early abort and Naive evaluation} \label{sec:prereq:fes:early_naive_eval}

\comment{Unclear section; what is early abort and what is naive evaluation --- and how does each affect complexity?}

The complexity of $O(2m2^n)$ bit operations is neither what the FES authors claim, \cite{cryptoeprint:2010/313, cryptoeprint:2013/436}, nor what Dinur necessitates, \cite{cryptoeprint:2021/578}. Using the procedure described earlier, further ideas may be used for a more efficient exhaustive search. Using the parallelization already mentioned, the authors of \cite{cryptoeprint:2010/313, cryptoeprint:2013/436} note that using an early abort strategy alongside the recursive algorithm already mentioned yields an algorithm which finds all common zeroes of $m$ polynomials in $log_2n \cdot 2^{n+2}$ bit operations. This early abort strategy is essentially to compute the common zeroes of $k$ polynomials (with a well-chosen $k$) in parallel, followed by then sequentially computing the common zeroes of the remainder of the polynomials. This last part is what \cite{cryptoeprint:2010/313, cryptoeprint:2013/436} denotes as \textit{candidate checking} as it boils down to checking only the common zeroes of the first $k$ polynomials, as any other evaluation point trivially cannot be a common zero of all $m$ polynomials.

\subsubsection{Partial evaluation and FES}

Two important ideas for obtaining a sufficient parallelization of the combined procedure (in practice) from \cref{sec:prereq:fes:exh_g_code} and \cref{sec:prereq:fes:early_naive_eval}; using bit-vectors for collecting bit operations of multiple polynomials in the system as well as \textit{partial evaluation}. By fixing $k$ variables, say $x_{n-k}$ to $x_{n-1}$, $2^k$ new systems may be obtained in which each $p_i \in \mathcal{P}$ is partially evaluated on some corresponding permutation of $k$ bits.

\comment{Explain what ``fixing'' means.}

The approach used by the authors of \cite{cryptoeprint:2010/313, cryptoeprint:2013/436} was therefore to select $w$ polynomials, fixing $k$ variables and producing $2^k$ smaller systems of $w$ polynomials which are searched using the recursive procedure described in \cref{sec:prereq:fes:exh_g_code}. Any common zero of these systems is then checked against the remaining $m-w$ polynomials. This approach achieves the proclaimed complexity of $2d\cdot\log_2 n \cdot 2^n$, as proved in \cite{cryptoeprint:2010/313}, which consequently is the one Dinur requires for his polynomial-method algorithm of \cite{cryptoeprint:2021/578}.

\subsection{Polynomial interpolation} \label{sec:prereq:poly_interp}
Polynomial interpolation is quite an important concept in terms of Dinur's polynomial-method algorithm. The Möbius transform, as described in \cite{joux2009algorithmic}, enables obtaining the algebraic normal form (ANF) of a boolean function using its truth table as input. The algebraic normal form of a boolean function $f$ on $n$ variables is written
$$
    f(x_0, \dots x_{n - 1}) = \bigoplus_{(a_0,\dots a_{n - 1}) \in \eff_2^n} g(a_0, \dots a_{n - 1}) \prod_i x_i^{a_i}
$$
where $g$ is the Möbius transform. The Möbius transform $g$ is a boolean function itself and may therefore be implemented using bit operations. The ANF of $f$ may be represented with a bit vector of length $2^n$. A closer inspection of the relationship between $g$ and $f$ further reveals that the Möbius transform can be computed using the formula
$$
    f(x_0, \dots x_{n - 1}) = f^{(0)}(x_0, \dots x_{n - 2}) \oplus f^{(1)}(x_0, \dots x_{n - 2}) \cdot x_{n - 1}
$$
where $f^{(0)}$ and $f^{(1)}$ are defined as 
\begin{equation*}
    \begin{split}
        f^{(0)}(x_0, \dots x_{n - 2}) &= f(x_0, \dots x_{n - 2}, 0), \\
        f^{(1)}(x_0, \dots x_{n - 2}) &= f(x_0, \dots x_{n - 2}, 0) \oplus f(x_0, \dots x_{n - 2}, 1).
    \end{split}
\end{equation*}
Now, a nice property of the Möbius transform is that it is an involution, and so it computes both the ANF of a boolean function/polynomial $f$ via its truth table but may also use the polynomial as input and obtain its truth table, i.e. a full evaluation on its input-space.

\begin{figure}[ht]
    \begin{alg}
        \caption{MOB\_TRANSFORM($S$, $n$)}
        \KwIn{The truth table $S$ of the boolean function $f$ on $n$ variables, with $2^n$ entries.}
        \KwResult{The Möbius transform of $f$, contained in the original list of $S$.}
        \ForEach{$i = 0, \dots n - 1$}{
            $Sz \gets 2^i$\;
            $Pos \gets 0$\;
            \While{$Pos < 2^n$}{
                \ForEach{$j = 0, \dots Sz - 1$}{
                    $S[Pos + Sz + j] \gets S[Pos + j] \oplus S[Pos + Sz + j]$\;
                }
                $Pos \gets Pos + 2 \cdot Sz$\;
            }
        }
    \end{alg}
    \caption{Layout of an implementation of the Möbius transform.}
    \label{alg:mob}
\end{figure}

There are different ways of computing the Möbius transform in practice. One method is the \textit{in-place} implementation, in which the input truth table is overwritten in place. This variant is described \cref{alg:mob} and an example implementation may be found in \cite{joux2009algorithmic}, alongside a more detailed description of the Möbius transform and related transforms. It should also be noted that the algorithm in \cref{alg:mob} is consistent with the involution property of the Möbius transform.

Finally, using the Möbius transform for fully evaluating a polynomial may be a viable option, comparing it against naively evaluating the polynomial on all inputs one by one. The works of \cite{10.1007/978-3-642-21702-9_6} show how one may approach such use of the Möbius transform, using the constructs above in tandem, and argues for its complexity.

\subsubsection{Transformations on sparse data}
When interpolating polynomials, not all $2^n$ assignments of the $n$ variables are needed. In \cite{cryptoeprint:2021/578} it was noted that the Möbius transform algorithm may be adapted to work with 
$$
    \sum_{i = 0}^{d} \binom{n}{i}
$$
evaluations in the input array, for a degree $d \leq n$ polynomial of $n$ variables. For this to work, only evaluations of low hamming weight input vectors ($\leq d$) are used, instead of a full evaluation. It was also argued in \cite{cryptoeprint:2021/578} that such an adaptation would yield a time complexity of $\leq n \cdot \sum_{i = 0}^{d}\binom{n}{i}$ bit operations.

\subsection{Dinur's polynomial-method algorithm}
A specific instance of the polynomial-method type algorithms for solving multivariate quadratic polynomial systems (see \cref{sec:prereq:polymethod}) is the algorithm of \cite{cryptoeprint:2021/578}, due to Dinur. As the title of the thesis states, this will be the algorithm in focus.

\subsubsection{Notation} \label{sec:prereq:dinur:notation}
% Describe the set W
% Describe the funky down-arrow binomial thingy
% Describe z_i \leftarrow 0

For the following sub-section to make sense, some notation is due, all of which is borrowed from \cite{cryptoeprint:2021/578}.
\begin{itemize}
    \item Let $W^n_w$ be the set $\{x \in \{0, 1\}^n \mid HW(x) \leq w \}$, where $HW(x)$ is the hamming weight of a vector $x$. 
    \item Let $\binom{n}{\downarrow w} = \sum^w_{i = 0} \binom{n}{i}$. This is also the size of the set $W^n_w$.
    \item The subscript $z_i \leftarrow 0$ implies that a polynomial $p_{z_i \leftarrow 0}(y,z)$ has it's $i$th $z$-bit set to zero.
\end{itemize}

\subsubsection{Complexities}

With the concrete complexities of the algorithms in \cite{doi:10.1137/1.9781611974782.143, Williams2014ThePM} being larger than $2^n$, a concretely efficient algorithm for cryptographic purposes was yet to be seen before \cite{cryptoeprint:2021/578}. In 2021, Dinur formulated a polynomial-method algorithm to be applicable in cryptography in general, and specifically for cryptanalytic purposes. This meant that the non-asymptotic complexities ought to be good even for very large problem sizes, due to the natural parameter sizes in cryptology. The asymptotic complexities of the formerly mentioned algorithms of \cite{doi:10.1137/1.9781611974782.143, Williams2014ThePM} may therefore be better, while in a non-asymptotic context not yielding the exponential speedup over exhaustive search as advertised. The algorithm provided in \cite{cryptoeprint:2021/578} therefore has the interesting property of yielding exponential speedup over exhaustive search, even for very large problem sizes. In this vein, the algorithms of \cite{doi:10.1137/1.9781611974782.143, Williams2014ThePM} have been revealed to have a concrete complexity larger than $2^n$ for cryptography-relevant parameters.

From analysis, the algorithm in \cite{cryptoeprint:2021/578} is bound to 
$$
    n^2 \cdot 2^{0.815n}
$$ 
bit operations for systems of quadratic polynomials, and 
$$
    n^2 \cdot 2^{(1 - \frac{1}{2.7d})}n
$$ 
for systems with degree $d > 2$ polynomials. This thesis will focus on the quadratic case, as this is the most relevant variant for cryptography. As a cryptanalytic tool, the algorithm was estimated to reduce the security margins of cryptographic schemes like HFE and UOV, however, some MQ-based schemes have resisted attacks using this algorithm. One downside to polynomial-method algorithms in general is memory usage. The spatial complexity of this algorithm is therefore also quite vast and was shown to be reducible to around 
$$
    n^2 \cdot 2^{0.63n}
$$ 
bits for quadratic polynomials systems. These complexities were proven in \cite{cryptoeprint:2021/578} as well.

\subsubsection{The algorithm} \label{sec:prereq:dinur_alg}
\begin{defn}[Isolated Solutions] \label{def:isolated_sol}
    Let $\hat{x} = (\hat{y}, \hat{z})$ be a solution to the polynomial system $\mathcal{P}$, where $\hat{x}$ is split into two parts via the variable partition $(\hat{y}, \hat{z})$. The solution, $\hat{x}$, is called \textit{isolated} if for any $\hat{z}' = \hat{z}$, $(\hat{y}, \hat{z}')$ is not a solution to $\mathcal{P}$.
\end{defn}

At its core, the algorithm from \cite{cryptoeprint:2021/578} is quite simple. The essential idea is to use smaller systems of polynomials to look for solutions. These smaller systems, of course, need a certain structure to provide the most relevant solutions for the remainder of the algorithm. To avoid brute-forcing these smaller systems, the idea is to divide any input $\mathbf{x} = \{x_0, \dots x_{n - 1}\}$ into two parts, allowing for obtaining and iterating \textit{isolated solutions} (\cref{def:isolated_sol}). The isolated solutions are then later used to obtain actual solutions for the system $\mathcal{P}$. The details of Dinur's polynomial-method solver can be seen in \cref{alg:solve}, with sub-procedures in \cref{alg:output} and \cref{alg:uvalue}.

\paragraph{Solving systems using the polynomial-method.} Looking at \cref{alg:solve}, one of the first things to done is preprocessing. This step is rather simple, as it essentially involves linearizing any quadratic terms in the polynomials $p_i \in \mathcal{P}$. This linearization removes any quadratic term, and adds to the polynomial a corresponding linear term in the same variable. This way, the FES procedure (\cref{alg:uvalues:bruteforce} in \cref{alg:uvalue}) is fed a system in the correct format. This part can of course be mitigated by feeding \cref{alg:solve} a system $\mathcal{P}$ in the correct format to begin with, however, to be thorough it was added as part of the procedure here. This is a less interesting step, but an explanation can be found in \cref{sec:prereq:fes}. Originally, in \cite{cryptoeprint:2021/578}, this step is not present, however, with the more practical approach taken here it seems reasonable to include it.

After preprocessing, and initializing $\ell$ and the $PotentialSolutions$ list, \cref{alg:solve} goes on to the vital part: Computing and checking potential solutions. The main loop of the algorithm starts out by generating a \textit{uniformly random full rank (rank $\ell$) binary matrix}, $A$, for computing the new system $\Tilde{\mathcal{P}}_k$. Requiring the matrix $A$ to be full rank was merely done in \cite{cryptoeprint:2021/578} for ease of analysis. Given the large degrees of identifying polynomials for larger polynomial systems (see \cref{sec:prereq:polymethod}), the approach in polynomial-methods is typically to obtain a \textit{probabilistic polynomial}, $\Tilde{F}$, of a similar system. Creating the system $\Tilde{\mathcal{P}}_k$ in the way described in \cref{alg:solve} ensures that its identifying polynomial, $\Tilde{F}$, and the identifying polynomial of the original system, $F$, agree on all solutions to the system $\mathcal{P}$. Even though $F$ and $\Tilde{F}$ agree on solutions to $\mathcal{P}$, the case where $F(\hat{\mathbf{x}}) = 0$ imposes $P[\Tilde{F}(\hat{\mathbf{x}}) = 0] \geq 1 - 2^{-\ell}$ for some $\hat{\mathbf{x}} \in \{0,1\}^{n - n_1}$, as proven in \cite{cryptoeprint:2021/578}. Then $F$ is of degree $d \cdot m$ and $\Tilde{F}$ is of degree $d \cdot \ell$, where $\ell < m$.

The solutions with the potential of being a common zero for the system $\mathcal{P}$ are output by the subprocedure \texttt{OUTPUT\_POTENTIALS} from \cref{alg:output} in \cref{alg:solve:output}. Once a set of the solutions to $\Tilde{\mathcal{P}}_k$ have been obtained, the history of previously obtained solutions (previous $\Tilde{E}_i$ for $i = 0, \dots k - 1$) is checked for overlaps with the freshly found solutions. The historic solutions are checked at \cref{alg:solve:check_history} and on. To avoid enumerating the whole search space (defeating the point of this algorithm) of $n$-bit solutions, the algorithm uses the parameter $n_1 < n$ to split solutions, $\mathbf{x}$, into $\mathbf{x} = (\mathbf{y}, \mathbf{z})$ where the $n - n_1$ initial bits are the $\mathbf{y}$-bits, and the $n_1$ latter bits are $\mathbf{z}$-bits. Splitting solutions for the system $\Tilde{\mathcal{P}}_k$ into two parts allows searching for \textit{isolated solutions} (\cref{def:isolated_sol}). Searching for isolated solutions compresses the space that needs to be searched with quite a large amount, assuming isolated solutions exist for the parameter $n_1$. It should be noted that some other literature describes algorithms similar in vein, however, these typically search isolated solutions over the identifying polynomial $F$ of the original system. The algorithm from \cite{cryptoeprint:2021/578} enumerates the isolated solutions of $\Tilde{F}$ and hence those of the randomly generated systems $\Tilde{E}_i$ for $i = 0, \dots k$. Due to the relationship of solution for $F$ and $\Tilde{F}$, any isolated solution found for $\Tilde{F}$ is not guaranteed to one for $F$. For $n_1 = \ell - 1$, an isolated solution $\hat{\mathbf{x}}$ for $F$ partitioned on $n_1$ is also an isolated solution for $\Tilde{F}$ with probability $\geq 1 - 2^{n_1 - \ell} = \frac{1}{2}$, as proven in \cite{cryptoeprint:2021/578}. Doing this will inherently make the interpolation and summation processes of \cref{alg:output} more efficient, as the polynomial being interpolated is reduced from degree $d \cdot m$ (for $F$) to $d \cdot \ell$ (for $\Tilde{F}$).

Because the isolated solutions obtained in the \textit{CurrPotentialSolutions} list (and the historic solutions; \textit{PotentialSolutions}) are not guaranteed to be isolated in $F$ (and hence not guaranteed to be a common zero of $\mathcal{P}$), the algorithm makes use of the history to minimize complexity. Ideally, every isolated solution could be tested on $\mathcal{P}$, however, considering that testing a single solution requires $\binom{n}{\downarrow d}$ bit operations this may end up quite expensive. Therefore, \cref{alg:solve:test_sol} is only ever executed once a candidate solution has appeared twice throughout the algorithm's run-time. In \cite{cryptoeprint:2021/578}, it is further argued that it is unlikely for an \textit{incorrect} candidate solution to appear more than once during solve-time. This limitation on testing solutions further improves the concrete complexity of the algorithm, when compared to the like-minded polynomial-method algorithms of the time.
\begin{figure}[ht]
    \centering
    \begin{alg}
        \caption{SOLVE($\mathcal{P}$, $m$, $n$, $n_1$)}
        \label{alg:solve}
        \KwIn{$\mathcal{P}$: $\{p_j(\mathbf{x})\}_{i = 0}^{m-1}$, $m$: Integer, $n$: Integer, $n_1$: Integer}
        \KwResult{A solution to the system $\mathcal{P}$} \label{alg:solve:matrix}
        PREPROCESS($\mathcal{P}$)\; \label{alg:solve:preprocess}
        $\ell \gets n_1 + 1$\;
        $PotentialSolutions \gets []$\;
        \ForEach{$k = 0,\dots$}{
            $A \gets \text{MATRIX($l$, $m$)}$\;
            $\Tilde{\mathcal{P}}_k \gets \{\sum_{j = 0}^{m-1}A_{i,j} \cdot p_j(\mathbf{x})\}^{\ell - 1}_{i = 0}$\; \label{alg:solve:e_k}
            $w \gets (\sum_{i=0}^{\ell - 1}\Tilde{\mathcal{P}}_k\text{.degrees()[}i\text{])} - n_1$\; \label{alg:solve:w}
            $CurrPotentialSolutions \gets$ OUTPUT\_POTENTIALS($\Tilde{\mathcal{P}}_k$, $n$, $n1$, $w$)\; \label{alg:solve:output}
            $PotentialSolutions[k] \gets CurrPotentialSolutions$\;
            \ForEach{$\hat{y} \in \{0,1\}^{n - n1}$}{ \label{alg:solve:check_history}
                \If{$CurrPotentialSolutions[\hat{y}][0] = 1$}{
                    \ForEach{$k_1 = 0, \dots k - 1$}{
                        \If{$CurrPotentialSolutions[\hat{y}] = PotentialSolutions[k_1][\hat{y}]$}{
                            $sol \gets \hat{y}\parallel CurrPotentialSolutions[\hat{y}]$\;
                            \If{TEST\_SOLUTION($\mathcal{P}$, $sol$)}{ \label{alg:solve:test_sol}
                                \Return $sol$\;
                            }
                        }
                    }
                }
            }
        }
    \end{alg}
    \caption{The top-level procedure of Dinur's polynomial-method algorithm.}
\end{figure}

\paragraph{Outputting potential solutions.} As already mentioned, the algorithm seeks \textit{candidate} solutions obtained from smaller systems $\Tilde{\mathcal{P}}_k$ (for some $k = 0, 1,\dots $). In any iteration of the main loop of \cref{alg:solve}, the \texttt{OUTPUT\_POTENTIALS} function computes candidate solutions through the polynomials $U_i$, for $i = 0, \dots n_1$. These polynomials are constructed as such: 
$$
U_0(\mathbf{y}) = \sum_{\hat{\mathbf{z}} \in \{0,1\}^{n_1}} \Tilde{F}(\mathbf{y}, \hat{\mathbf{z}}),
$$
and
$$
U_i(\mathbf{y}) = \sum_{\hat{\mathbf{z}} \in \{0,1\}^{n_1 - 1}} \Tilde{F}_{z_i \leftarrow 0}(\mathbf{y}, \hat{\mathbf{z}})
$$
for $i = 1, \dots n_1$. As is described in \cite{cryptoeprint:2021/578}, constructing polynomials this way, allows for enumerating the isolated solutions, partitioned by $(\mathbf{y},\mathbf{z})$ and in this case $n_1$. That is, for each input $\hat{\mathbf{y}}$ we may compute the remaining $n_1$ bits of a solution using these sums, assuming $\hat{\mathbf{y}}$ belongs to an isolated solution. In his paper, Dinur proves that if $(\hat{\mathbf{y}}, \hat{\mathbf{z}})$ is an isolated solution to $\Tilde{\mathcal{P}}$, then $U_0(\hat{\mathbf{y}}) = 1$ and $U_i(\hat{\mathbf{y}}) = z_i + 1$, for $i = 1, \dots, n_1$. Therefore, by computing the sums (or parities) $U_i$ for $i = 0, \dots n_1$ we may easily retrieve the latter $n_1$ bits of an isolated solution, assuming $U_0(\hat{\mathbf{y}}) = 1$. Going through solutions and recovering the $z_i$ bits occurs in the latter loop of \cref{alg:output} (\cref{alg:output:recover}).

Looking at how these polynomials/sums are constructed we see that they require quite a large amount of computation, should they be searched exhaustively, as they each require an exponential amount of evaluations of the polynomial $\Tilde{F}$. To avoid this, the algorithm instead uses the \texttt{COMPUTE\_U\_VALUES} procedure to compute interpolation points. The procedure stores these interpolation points in the $V$ and $ZV$ arrays, in \cref{alg:output:uvalues}, with $V$ being the values used for interpolating $U_0$ and $ZV$ being the values used for interpolating $U_i$ for $i = 1, \dots, n_1$. Now, as discussed in \cref{sec:prereq:poly_interp} a boolean polynomial can be interpolated from its evaluations (\textit{truth-table}, essentially) using the Möbius transform. Instead of \texttt{COMPUTE\_U\_VALUES} having to compute entire truth-tables, we may modify the Möbius transform to enable, what we denote, \textit{sparse interpolation} of a polynomial. This modification is described in \cite{10.1007/978-3-642-21702-9_6}. In order to minimize the cost of sparsely computing $U$ evaluations, Dinur proved (in \cite{cryptoeprint:2021/578}) that we may do the interpolations in \cref{alg:output:mob_0} and \cref{alg:output:mob_1} using solutions to $\Tilde{\mathcal{P}}$ in the set and $W^{n - n_1}_{w + 1} \times \{0, 1\}^{n_1}$. These solutions evaluations are stored in variables $V$ and $ZV$.

Once the $U_i$ polynomials have been interpolated in \cref{alg:output:mob_0} and \cref{alg:output:mob_1} they may be used for an exhaustive search. At this point, instead of evaluating these polynomials directly, we use the fact that the Möbius transform is its inverse (\cref{sec:prereq:poly_interp}) by giving the $U$ polynomials as inputs to the transform procedure. For this to work, we of course need a way of storing the polynomial in an array structured similarly to the truth tables. This brute force approach can be seen in \cref{alg:output:mob_2}.


\begin{figure}[ht]
    \centering
    \begin{alg}
        \caption{OUTPUT\_POTENTIALS($\Tilde{\mathcal{P}}$, $n$, $n_1$, $w$)}
        \label{alg:output}
        \KwIn{$\Tilde{\mathcal{P}}$: $\{r_i(\mathbf{x})\}_{i = 0}^{\ell - 1}$, $n$: Integer, $n_1$: Integer, $w$: Integer}
        \KwResult{A two-dimensional list of size $2^{n - n_1} \times (n_1 + 1)$ containing the $z_i$ bits, $y$ bits and $U_0(y)$ bit.}
        $(V, ZV[0,\dots(n_1 - 1)]) \gets $ COMPUTE\_U\_VALUES($\Tilde{\mathcal{P}}$, $n$, $n_1$, $w$)\; \label{alg:output:uvalues}
        $U_0 \gets$ MOB\_TRANSFORM($V$[$0\dots |W^{n - n_1}_{w}| - 1$], $n - n_1$)\; \label{alg:output:mob_0}
        \ForEach{$i = 1\dots n_1$}{
            $U_i \gets$ MOB\_TRANSFORM($ZV$[$i$][$0, \dots, |W^{n - n1}_{w + 1}| - 1$], $n - n_1$)\; \label{alg:output:mob_1}
        }
        $Evals[0\dots n_1][0\dots 2^{n - n1} - 1] \gets \{0\}$\;
        \ForEach{$i = 0\dots n_1$}{
            $Evals[i][0\dots 2^{n - n1} - 1] \gets$ MOB\_TRANSFORM($U_i$.as\_array(), $n - n_1$)\; \label{alg:output:mob_2}
        }
        $Out[0\dots 2^{n - n1} - 1][0\dots n_1] \gets \{0\}$\;
        \ForEach{$\hat{y} \in \{0,1\}^{n - n_1}$}{ \label{alg:output:recover}
            \If{$Evals[0][\hat{y}] = 1$}{
                $Out[\hat{y}][0] \gets 1$\;
                \ForEach{$i = 1\dots n_1$}{
                    $Out[\hat{y}][i] \gets Evals[i][\hat{y}] + 1$\;
                }
            }
        }
        \Return $Out$\;
    \end{alg}
    \caption{The subprocedure for retrieving candidate solutions.}
\end{figure}

\paragraph{Computing interpolation points for the $U$ polynomials.} The points needed for sparsely interpolating the $U$ polynomials may not be chosen freely. According to the proof due to Dinur in \cite{cryptoeprint:2021/578}, the necessary points depend on the hamming weight of the initial $n - n_1$ bits of the solution to $\Tilde{\mathcal{P}}$. Dinur essentially states that in order to interpolate $U_0$ for some system $\Tilde{\mathcal{P}}$, we need all solutions $(\hat{\mathbf{y}}, \hat{\mathbf{z}})$ where the hamming weight of the $\hat{\mathbf{y}}$ vector is less than $w = d_{\Tilde{F}} - n_1$ (\cref{alg:solve:w} in \cref{alg:solve}). Likewise, to interpolate $U_i$, for $i = 1, \dots, n_1$, we need all solutions where the hamming weight of the $\hat{\mathbf{y}}$-bits is less than $w + 1$.

To obtain solutions for $\Tilde{\mathcal{P}}$, usable for the interpolation in \texttt{OUTPUT\_POTENTIALS}, Dinur specifies using the FES procedure (\cref{sec:prereq:fes}), denoted \texttt{BRUTEFORCE} at \cref{alg:uvalues:bruteforce} in \cref{alg:uvalues}. The idea behind using this procedure is that we may obtain the interpolation points with complexity 
$$
    2d \cdot \log n \cdot 2^{n_1} \cdot \binom{n - n_1}{\downarrow w}.
$$
However, as the FES procedure itself is not designed to iterate through sparse sets of inputs, such as $W^{n - n_1}_w \times \{0, 1\}^{n_1}$, an overhead is incurred. The $n_1$ latter bits can be iterated using the normal gray code traversal of FES, meaning a penalty is incurred for each $2^{n_1}$ iteration. A conservative estimate by Dinur confines this to an amortized penalty of $2^{- n_1} \cdot n$ over the FES procedure. As the complexities derived for Dinur's polynomial-method algorithm assume cryptographically relevant parameter sizes, $2^{n_1} \gg n$ means the overhead is negligible.

Once the relevant solutions for $\Tilde{\mathcal{P}}$ have been computed by, the procedure goes on to compute the actual interpolation points for the $U$ polynomials. The loop at \cref{alg:uvalues:sum} (\cref{alg:uvalues}) computes the evaluations for the $U$ polynomials by iterating through all solutions and filtering depending on the values of the $\Hat{y}$ and $\Hat{z}$ bits. As the $U_0$ polynomial can be interpolated from its values in $W^{n - n_1}_w \times \{0, 1\}^{n - n_1}$, \cref{alg:uvalues} simply checks whether or not the current $\Hat{y}$ bits have a hamming weight $\leq w$, before summing and storing. Likewise, to filter out solutions for some $U_i$, the procedure inspects the $\Hat{z}$-bits and checks whether or not $z_i = 0$ (as is necessary by the construction of $U_i$s).

It should be noted that the gray code traversal of the FES sub-procedure could also be reimplemented using monotone gray codes instead of traditional binary reflected gray codes, as noted by Dinur in \cite{cryptoeprint:2021/578}. This way, the gray code traversal is done in an \textit{almost} increasing order according to the hamming weight.

\begin{figure}[ht]
    \centering
    \begin{alg}
        \caption{COMPUTE\_U\_VALUES($\Tilde{\mathcal{P}}$, $n$, $n_1$, $w$)} \label{alg:uvalue}
        \label{alg:uvalues}
        \KwIn{$\Tilde{\mathcal{P}}$: $\{r_i(\mathbf{x})\}_{i = 0}^{\ell - 1}$, $n_1$: Integer, $w$: Integer}
        \KwResult{Lists $V$ and $ZV$ containing evaluations of $U_i(y), \forall i \in \{0, \dots n_1\}, \forall y \in \{y \mid y \in \{0,1\}^{n - n_1}, hw(y) \leq w\}$}
        $Sols[0\dots L - 1] \gets$ BRUTEFORCE($\Tilde{\mathcal{P}}$, $n$, $n1$, $w + 1$)\; \label{alg:uvalues:bruteforce}
        $V[0\dots |W^{n - n1}_w| - 1] \gets \{0\}$\;
        $ZV[0\dots n_1][0\dots |W^{n - n_1}_{w + 1}| - 1] \gets \{0\}$\;
        \ForEach{$s \in Sols$}{ \label{alg:uvalues:sum}
            $\hat{y}, \hat{z} \gets s[0\dots n - n_1 - 1], s[n - n_1 \dots n - 1]$\;
            \If{$\text{HAMMING\_WEIGHT(} \hat{y} \text{)} \leq w$}{
                $idx \gets$ INDEX\_OF($\hat{y}$, $n - n_1$, $w$)\;
                $V[idx]\pp$\;
            }
            \ForEach{$i = 1\dots n_1 $}{
                \If{$z_i = 0$}{
                    $idx \gets$ INDEX\_OF($\hat{y}$, $n - n_1$, $w + 1$)\;
                    $ZV[i][idx]\pp$\;
                }
            }
        }
        \Return $V, ZV[1\dots n_1]$\;
    \end{alg}
    \caption{The subprocedure for computing interpolation points.}
\end{figure}


\subsection{Platform and architecture}

This section serves to describe the platform and architecture used for building, testing, and benchmarking the programs written for this thesis. For a description of the actual implementation see \cref{sec:impl}.

The compiler and operating systems used in this thesis are GCC and GNU/Linux. Therefore, running a non-GNU/Linux operating system or compiling using a non-GCC C/C++ compiler is not guaranteed to work. In case you want to run it in a containerized environment, refer to the accompanying Dockerfile. Although most modern CISC CPUs are based on 64-bit instructions, compilation targets also exist for 32, 16, and 8-bit platforms.
\td{ADD DOCKERFILE TO REPOSITORY}

Some of the most important instruction sets present today, other than the common CPU instructions like addition and move/load, are SIMD (Single Instruction Multiple Data) instruction sets. Two such instruction sets are the SSE2 (Steaming SIMD Extensions), AVX (Advanced Vector Extensions), and AVX2 instruction sets. These are all present on modern Wintel-compatible CPUs, however, some also include the AVX512 instruction set. The compilation targets in this thesis only support up to AVX/AVX2. These instruction sets allow using 128-bit and 256-bit vector registers for computation on multiple data elements in one instruction. AVX512 enables further support for 256-bit vector registers while also supporting 512-bit vector registers.

The AVX/AVX2 instructions operate on the \texttt{ymm}[0-15] 256-bit registers, while the SSE2 instructions operate on the \texttt{xmm}[0-15] 128-bit registers. These instruction sets support loads/stores, bit-wise operations, addition, subtraction, shuffling, and much more. The data packed into a vector may be both floating point data or integer data, however, in this project only integer data is necessary. To examine the full extent of these instruction sets, as well as their latencies and throughputs, refer to the respective documentation at the CPU manufacturer (such as Intel or AMD).

The parts of the code that make use of vector instructions do not use GCCs inline assembly or simply assembly files. Instead, the programs using vector instructions make use of GCC intrinsic functions for these instruction sets, such as those documented in \cite{IntelIntr}.

\td{NOTE ABOUT MULTICORE?}
\newpage
