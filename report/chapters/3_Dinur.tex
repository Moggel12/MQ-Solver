\chapter{Dinur's polynomial-method solver} \label{sec:dinur}

A specific instance of the polynomial method type algorithms for solving multivariate quad\-ratic polynomial systems (see \cref{sec:prereq:polymethod}) is the algorithm of \cite{eurocrypt-2021-30841}, due to Dinur. As the title of the thesis states, this will be the algorithm in focus. In \cite{doi:10.1137/1.9781611974782.143} and \cite{Williams2014ThePM}, alternative polynomial method algorithms are described. These algorithms served as inspiration for Dinur's solver.

\section{Notation} \label{sec:dinur:notation}

For the following sub-section to make sense, some notation is due, all of which is borrowed from \cite{eurocrypt-2021-30841}.
\begin{itemize}
    \item Let $W^n_w$ be the set $\{\mathbf{x} \in \{0, 1\}^n \mid HW(\mathbf{x}) \leq w \}$, where $HW(\mathbf{x})$ is the hamming weight of a vector $\mathbf{x}$. 
    \item Let $\binom{n}{\downarrow w} = \sum^w_{i = 0} \binom{n}{i}$. This is also the size of the set $W^n_w$.
    \item The subscript $z_i \leftarrow 0$ implies that a polynomial $p_{z_i \leftarrow 0}(\mathbf{y},\mathbf{z})$ has it's $i$th $z$-variable ($z_i \in \mathbf{z}$) set to zero.
\end{itemize}

\section{Complexities}

With the concrete complexities of the algorithms in \cite{doi:10.1137/1.9781611974782.143, Williams2014ThePM} being larger than $2^n$, a concretely efficient algorithm for cryptographic purposes was yet to be seen before \cite{eurocrypt-2021-30841}. In 2021, Dinur formulated a polynomial-method algorithm to be applicable in cryptography in general, and specifically for cryptanalytic purposes. This meant that the non-asymptotic complexities ought to be good even for very large problem sizes, due to the natural parameter sizes in cryptology. The asymptotic complexities of the formerly mentioned algorithms of \cite{doi:10.1137/1.9781611974782.143, Williams2014ThePM} may therefore be better, while in a non-asymptotic context not yielding the exponential speedup over exhaustive search as advertised. The algorithm provided in \cite{eurocrypt-2021-30841} therefore has the interesting property of yielding exponential speedup over exhaustive search, even for very large problem sizes. In this vein, the algorithms of \cite{doi:10.1137/1.9781611974782.143, Williams2014ThePM} have been revealed to have a concrete complexity larger than $2^n$ for cryptography-relevant parameters.

From analysis, the algorithm in \cite{eurocrypt-2021-30841} is bound to 
$$
    n^2 \cdot 2^{0.815n}
$$ 
bit operations for systems of quadratic polynomials, and 
$$
    n^2 \cdot 2^{(1 - \frac{1}{2.7d})}n
$$ 
for systems with degree $d > 2$ polynomials. This thesis will focus on the quadratic case, as this is the most relevant variant for cryptography. As a cryptanalytic tool, the algorithm was estimated to reduce the security margins of cryptographic schemes like HFE and UOV, however, some MQ-based schemes have resisted attacks using this algorithm. One downside to polynomial-method algorithms in general is memory usage. The spatial complexity of this algorithm is therefore also quite vast and was shown to be reducible to around 
$$
    n^2 \cdot 2^{0.63n}
$$ 
bits for quadratic polynomials systems. These complexities were proven in \cite{eurocrypt-2021-30841} as well.

\section{The algorithm} \label{sec:dinur:dinur_alg}
\begin{defn}[Isolated Solutions] \label{def:isolated_sol}
    Let $\hat{\mathbf{x}} = (\hat{\mathbf{y}}, \hat{\mathbf{z}})$ be a solution to the polynomial system $\mathcal{P}$, where $\hat{x}$ is split into two parts via the variable partition $(\hat{\mathbf{y}}, \hat{\mathbf{z}})$. The solution, $\hat{\mathbf{x}}$, is called \textit{isolated} if for any $\hat{\mathbf{z}}' = \hat{\mathbf{z}}$, $(\hat{\mathbf{y}}, \hat{\mathbb{z}}')$ is not a solution to $\mathcal{P}$.
\end{defn}

At its core, the algorithm from \cite{eurocrypt-2021-30841} is quite simple. The essential idea is to use smaller systems of polynomials to look for solutions. These smaller systems, of course, need a certain structure to provide the most relevant solutions for the remainder of the algorithm. To avoid brute-forcing these smaller systems, the idea is to divide any input $\mathbf{x} = (x_0, \dots x_{n - 1})$ into two parts, allowing for obtaining and iterating \textit{isolated solutions} (\cref{def:isolated_sol}). The isolated solutions are then later used to obtain actual solutions for the system $\mathcal{P}$. The details of Dinur's polynomial-method solver can be seen in \cref{alg:solve}, with sub-procedures in \cref{alg:output} and \cref{alg:uvalue}.

For clarity, let $p$ be a polynomial over $\eff_2[x_0,x_1,x_2]$. Now, define a \textit{partitioning} of the variables $\mathbf{x} = (\mathbf{y}, \mathbf{z}) = (y_0, y_1, z_0)$, such that $x_0 = y_0, x_1 = y_1, x_2 = z_0$. Instead of searching for solutions in the space of all three variables, Dinur's solver searches for \textit{isolated solutions}, using the partitioning above (\cref{def:isolated_sol}), meaning that only assignments for the $\mathbf{y}$ variables need to be searched. Let $\hat{\mathbf{x}} = (1,1,0)$ be a solution to $p$. This solution is isolated only if $p(1,1,1) = 1$, as this is not a solution to $p$ (see \cref{sec:prereq:problem}). It may still be the case that $\hat{\mathbf{x}}' = (0,1,0)$ is a solution, as the $\mathbf{y}$ variables are different in $\hat{\mathbf{x}}$ from $\hat{\mathbf{x}}'$. Using certain constructs, the algorithm may then \textit{recover} the $\mathbf{z}$ bits of an isolated solution, thereby obtaining all $n$ variables assignments of $\hat{\mathbf{x}} = (x_0, \dots x_{n - 1})$. 

\begin{figure}[t]
    \centering
    \begin{alg}
        \caption{SOLVE($\mathcal{P}$, $m$, $n$, $n_1$)}
        \label{alg:solve}
        \KwIn{$\mathcal{P}$: $\{p_j(\mathbf{x})\}_{i = 0}^{m-1}$, $m$: Integer, $n$: Integer, $n_1$: Integer}
        \KwResult{A solution to the system $\mathcal{P}$} \label{alg:solve:matrix}
        PREPROCESS($\mathcal{P}$)\; \label{alg:solve:preprocess}
        $\ell \gets n_1 + 1$\;
        $PotentialSolutions \gets []$\;
        \ForEach{$k = 0,\dots$}{
            $A^{(k)} \gets \text{MATRIX($l$, $m$)}$\;
            $\Tilde{\mathcal{P}}_k \gets \{\sum_{j = 0}^{m-1}A^{(k)}_{i,j} \cdot p_j(\mathbf{x})\}^{\ell - 1}_{i = 0}$\; \label{alg:solve:e_k}
            $w \gets (\sum_{i=0}^{\ell - 1}\Tilde{\mathcal{P}}_k\text{.degrees()[}i\text{])} - n_1$\; \label{alg:solve:w}
            $CurrPotentialSolutions \gets$ OUTPUT\_POTENTIALS($\Tilde{\mathcal{P}}_k$, $n$, $n1$, $w$)\; \label{alg:solve:output}
            $PotentialSolutions[k] \gets CurrPotentialSolutions$\;
            \ForEach{$\hat{y} \in \{0,1\}^{n - n1}$}{ \label{alg:solve:check_history}
                \If{$CurrPotentialSolutions[\hat{y}][0] = 1$}{
                    \ForEach{$k_1 = 0, \dots k - 1$}{
                        \If{$CurrPotentialSolutions[\hat{y}] = PotentialSolutions[k_1][\hat{y}]$}{
                            $sol \gets \hat{y}\parallel CurrPotentialSolutions[\hat{y}]$\;
                            \If{TEST\_SOLUTION($\mathcal{P}$, $sol$)}{ \label{alg:solve:test_sol}
                                \Return $sol$\;
                            }
                        }
                    }
                }
            }
        }
    \end{alg}
    \caption{The top-level procedure of Dinur's polynomial-method algorithm.}
\end{figure}

\paragraph{Solving systems using the polynomial method.} Inspecting \cref{alg:solve}, one of the first things to be done is preprocessing. This step is rather simple, as it essentially involves linearizing (see \cref{sec:prereq:partial_eval}) any quadratic terms in the polynomials $p_i \in \mathcal{P}$. This linearization removes any quadratic term and adds to the polynomial a corresponding linear term in the same variable. This way, the FES procedure (\cref{alg:uvalues:bruteforce} in \cref{alg:uvalue}) is fed a system in the correct format.

After preprocessing, and initializing $\ell$ and the $PotentialSolutions$ list, \cref{alg:solve} goes on to the vital part: Computing and checking potential solutions. The main loop of the algorithm starts by generating a \textit{uniformly random full rank (rank $\ell$) binary matrix}, $A^{(k)}$, for generating the new system $\Tilde{\mathcal{P}}_k$. Requiring the matrix $A^{(k)}$ to be full rank was merely done in \cite{eurocrypt-2021-30841} for ease of analysis. Given the large degrees of identifying polynomials for larger polynomial systems (see \cref{sec:prereq:polymethod}), the approach in polynomial-methods is typically to obtain a \textit{probabilistic polynomial}, $\Tilde{F}$, of a similar system. Creating the system $\Tilde{\mathcal{P}}_k$ in the way described in \cref{alg:solve} ensures that any assignment, $\hat{\mathbf{x}}$, such that $F(\hat{\mathbf{x}}) = 1$ also has $\Tilde{F}(\hat{\mathbf{x}}) = 1$. Recall from \cref{sec:prereq:polymethod} that $F(\hat{\mathbf{x}}) = 1$ states that $\hat{\mathbf{x}}$ is a solution to $\mathcal{P}$. Even though $F$ and $\Tilde{F}$ agree on solutions to $\mathcal{P}$, the case where $F(\hat{\mathbf{x}}) = 0$ imposes $P[\Tilde{F}(\hat{\mathbf{x}}) = 0] \geq 1 - 2^{-\ell}$ for some $\hat{\mathbf{x}} \in \{0,1\}^{n - n_1}$, as proven in \cite{eurocrypt-2021-30841}. For this reason, $\Tilde{F}(\hat{\mathbf{x}}) = 1$ does \textit{not} guarantee that $F(\hat{\mathbf{x}}) = 1$, i.e. a solution to $\Tilde{\mathcal{P}}$ is not guaranteed to be one for $\mathcal{P}$. Additionally, $F$ is of degree $d \cdot m$ and $\Tilde{F}$ is of degree $d \cdot \ell$, where $\ell < m$.

Searching for solutions is initially handled by searching for isolated solutions of $\Tilde{\mathcal{P}}$ instead of $\mathcal{P}$. As it is not guaranteed that a solution to $\Tilde{\mathcal{P}}$ is also a solution to $\mathcal{P}$, the procedure stores any isolated solutions found as \textit{potential solutions}. These potential solutions are computed by the sub-procedure \texttt{OUTPUT\_POTENTIALS()} through multiple rounds, with each round generating a new matrix $A^{(k)}$ and new random ''sub''-system, $\Tilde{\mathcal{P}}_k$. The \texttt{OUTPUT\_POTENTIALS()} procedure can be found in \cref{alg:output}. The potential solutions are computed by partitioning the $n$ variables using the parameter $n_1 < n$. Here, $\mathbf{x} = (\mathbf{y}, \mathbf{z})$ is the partitioning where the first $n - n_1$ variables are the $\mathbf{y}$ variables, and the latter $n_1$ variables are $\mathbf{z}$. Of course, this process of searching only for \textit{isolated} solutions can drastically compress the search space from the $2^n$ possible solutions. Further, the idea behind using $\Tilde{\mathcal{P}}_k$ is to get a faster interpolation and summation process in \cref{alg:output}, as $\Tilde{F}$ is of degree $ \leq d \cdot \ell < d \cdot m$. Of course, as already stated, the tradeoff is that potential solutions are not guaranteed to be solutions for $\mathcal{P}$. Specifically, by partitioning using $n_1 = \ell - 1$, an isolated solution $\hat{\mathbf{x}} = (\mathbf{\hat{y}}, \mathbf{\hat{z}})$ to $\mathcal{P}$ imposes that $\hat{\mathbf{x}}$ is an isolated solution to $\Tilde{\mathcal{P}}_k$ with probability $\geq 1 - 2^{n_1 - \ell} = \frac{1}{2}$ (proven in \cite{eurocrypt-2021-30841}). This idea of searching for isolated solutions was not a novel idea introduced by Dinur, however, the alternative algorithms that use isolated solutions all searched for them on the identifying polynomial, $F$, instead of $\Tilde{F}$.

As potential solutions are stored in each round, in order to check if one is a \textit{candidate solution} the procedure compares newly found potential solutions with those found in previous rounds. The solutions found in the current round are stored in the \texttt{CurrPotentialSolutions} list, and later added to the history of previous potential solutions in \textit{PotentialSolutions}. Any potential solution $(\hat{\mathbf{y}}, \hat{\mathbf{z}})$ found in more than one round is said to be a \textit{candidate}. Once a candidate solution has been found, the procedure tests the solution by evaluating it on the polynomial system. Since evaluating the system $\mathcal{P}$ on an assignment (\cref{alg:solve:test_sol} in \cref{alg:solve}) is an expensive operation, the idea of only checking potential solutions found more than once is used to minimize the concrete complexity in this area. In \cite{eurocrypt-2021-30841} it is also argued that having a \textit{candidate} (i.e. a potential solution found more than once) be an \textit{incorrect} candidate solution is unlikely.

\begin{figure}[t]
    \centering
    \begin{alg}
        \caption{OUTPUT\_POTENTIALS($\Tilde{\mathcal{P}}$, $n$, $n_1$, $w$)}
        \label{alg:output}
        \KwIn{$\Tilde{\mathcal{P}}$: $\{r_i(\mathbf{x})\}_{i = 0}^{\ell - 1}$, $n$: Integer, $n_1$: Integer, $w$: Integer}
        \KwResult{A two-dimensional list of size $2^{n - n_1} \times (n_1 + 1)$ containing the $z_i$ bits, $y$ bits and $U_0(y)$ bit.}
        $(V, ZV[0,\dots(n_1 - 1)]) \gets $ COMPUTE\_U\_VALUES($\Tilde{\mathcal{P}}$, $n$, $n_1$, $w$)\; \label{alg:output:uvalues}
        $U_0 \gets$ MOB\_TRANSFORM($V$[$0\dots |W^{n - n_1}_{w}| - 1$], $n - n_1$)\; \label{alg:output:mob_0}
        \ForEach{$i = 1\dots n_1$}{
            $U_i \gets$ MOB\_TRANSFORM($ZV$[$i$][$0, \dots, |W^{n - n1}_{w + 1}| - 1$], $n - n_1$)\; \label{alg:output:mob_1}
        }
        $Evals[0\dots n_1][0\dots 2^{n - n1} - 1] \gets \{0\}$\;
        \ForEach{$i = 0\dots n_1$}{
            $Evals[i][0\dots 2^{n - n1} - 1] \gets$ MOB\_TRANSFORM($U_i$.as\_array(), $n - n_1$)\; \label{alg:output:mob_2}
        }
        $Out[0\dots 2^{n - n1} - 1][0\dots n_1] \gets \{0\}$\;
        \ForEach{$\hat{y} \in \{0,1\}^{n - n_1}$}{ \label{alg:output:recover}
            \If{$Evals[0][\hat{y}] = 1$}{
                $Out[\hat{y}][0] \gets 1$\;
                \ForEach{$i = 1\dots n_1$}{
                    $Out[\hat{y}][i] \gets Evals[i][\hat{y}] + 1$\;
                }
            }
        }
        \Return $Out$\;
    \end{alg}
    \caption{The subprocedure for retrieving candidate solutions.}
\end{figure}

\paragraph{Outputting potential solutions.} As already mentioned, the algorithm seeks \textit{potential} solutions obtained from smaller systems $\Tilde{\mathcal{P}}_k$ (for some $k = 0, 1,\dots $). In any iteration of the main loop of \cref{alg:solve}, the \texttt{OUTPUT\_POTENTIALS} function computes potential solutions through the polynomials $U_i$, for $i = 0, \dots n_1$. These polynomials are constructed as such: 
$$
U_0(\mathbf{y}) = \sum_{\hat{\mathbf{z}} \in \{0,1\}^{n_1}} \Tilde{F}(\mathbf{y}, \hat{\mathbf{z}}),
$$
and
$$
U_i(\mathbf{y}) = \sum_{\hat{\mathbf{z}} \in \{0,1\}^{n_1 - 1}} \Tilde{F}_{z_{i - 1} \leftarrow 0}(\mathbf{y}, \hat{\mathbf{z}})
$$
for $i = 1, \dots n_1$. As is described in \cite{eurocrypt-2021-30841}, constructing polynomials this way, allows for enumerating the isolated solutions, partitioned by $(\mathbf{y},\mathbf{z})$ and in this case $n_1$. That is, for each input $\hat{\mathbf{y}}$ the remaining $n_1$ bits of a solution may be computed using these sums, assuming $\hat{\mathbf{y}}$ belongs to an isolated solution. In his paper, Dinur proves that if $(\hat{\mathbf{y}}, \hat{\mathbf{z}})$ is an isolated solution to $\Tilde{\mathcal{P}}_k$, then $U_0(\hat{\mathbf{y}}) = 1$ and $U_i(\hat{\mathbf{y}}) = z_{i - 1} + 1$, for $i = 1, \dots, n_1$. Therefore, by computing the sums (or parities) $U_i$ for $i = 0, \dots n_1$ latter $n_1$ bits of an isolated solution can be easily recovered, assuming $U_0(\hat{\mathbf{y}}) = 1$. Going through solutions and recovering the $z_{i - 1}$ bits occurs in the latter loop of \cref{alg:output} (\cref{alg:output:recover}).

Looking at how these polynomials/sums are constructed it is clear that they require quite a large amount of computation, should they be searched exhaustively, as they each require an exponential amount of evaluations of the polynomial $\Tilde{F}$. To avoid this, the algorithm instead uses the \texttt{COMPUTE\_U\_VALUES()} procedure to compute interpolation points. The procedure stores these interpolation points in the $V$ and $ZV$ arrays, in \cref{alg:output:uvalues}, with $V$ being the values used for interpolating $U_0$ and $ZV$ being the values used for interpolating $U_i$ for $i = 1, \dots, n_1$. Now, as discussed in \cref{sec:prereq:poly_interp} a boolean polynomial can be interpolated from its evaluations (\textit{truth-table}, essentially) using the Möbius transform. Instead of \texttt{COMPUTE\_U\_VALUES} having to compute entire truth-tables, the Möbius transform can be modified to enable, what is here denoted as, \textit{sparse interpolation} of a polynomial. This modification is described in \cite{fse-2011-23547}, and briefly in \cref{sec:prereq:poly_interp:sparse}. In order to minimize the cost of sparsely computing $U$ evaluations, Dinur proved (in \cite{eurocrypt-2021-30841}) that the interpolations in \cref{alg:output:mob_0} and \cref{alg:output:mob_1} can be performed using solutions to $\Tilde{\mathcal{P}}_k$ in the set and $W^{n - n_1}_{w + 1} \times \{0, 1\}^{n_1}$. These evaluations are stored in variables $V$ and $ZV$.

Once the $U$ polynomials have been interpolated in \cref{alg:output:mob_0} and \cref{alg:output:mob_1} they are exhaustively evaluated on the $2^{n - n_1}$ $\mathbf{y}$ variables. At this point, instead of evaluating these polynomials directly, the Möbius transform is used by giving the $U$ polynomials as inputs to the transform procedure. As is mentioned in \cref{sec:prereq:poly_interp}, the Möbius transform is its own inverse and can therefore be used for both interpolation and evaluation. For this to work, the polynomials need to be stored in an array structured similarly to the truth tables. This brute force approach can be seen in \cref{alg:output:mob_2}.

\begin{figure}[ht]
    \centering
    \begin{alg}
        \caption{COMPUTE\_U\_VALUES($\Tilde{\mathcal{P}}$, $n$, $n_1$, $w$)} \label{alg:uvalue}
        \label{alg:uvalues}
        \KwIn{$\Tilde{\mathcal{P}}$: $\{r_i(\mathbf{x})\}_{i = 0}^{\ell - 1}$, $n_1$: Integer, $w$: Integer}
        \KwResult{Lists $V$ and $ZV$ containing evaluations of $U_i(y), \forall i \in \{0, \dots n_1\}, \forall y \in \{y \mid y \in \{0,1\}^{n - n_1}, hw(y) \leq w\}$}
        $Sols[0\dots L - 1] \gets$ BRUTEFORCE($\Tilde{\mathcal{P}}$, $n$, $n1$, $w + 1$)\; \label{alg:uvalues:bruteforce}
        $V[0\dots |W^{n - n1}_w| - 1] \gets \{0\}$\;
        $ZV[0\dots n_1][0\dots |W^{n - n_1}_{w + 1}| - 1] \gets \{0\}$\;
        \ForEach{$s \in Sols$}{ \label{alg:uvalues:sum}
            $\hat{y}, \hat{z} \gets s[0\dots n - n_1 - 1], s[n - n_1 \dots n - 1]$\;
            \If{$\text{HAMMING\_WEIGHT(} \hat{y} \text{)} \leq w$}{
                $idx \gets$ INDEX\_OF($\hat{y}$, $n - n_1$, $w$)\;
                $V[idx]\pp$\;
            }
            \ForEach{$i = 1\dots n_1 $}{
                \If{$z_i = 0$}{
                    $idx \gets$ INDEX\_OF($\hat{y}$, $n - n_1$, $w + 1$)\;
                    $ZV[i][idx]\pp$\;
                }
            }
        }
        \Return $V, ZV[1\dots n_1]$\;
    \end{alg}
    \caption{The subprocedure for computing interpolation points.}
\end{figure}

\paragraph{Computing interpolation points for the $U$ polynomials.} The points needed for sparsely interpolating the $U$ polynomials may not be chosen freely. According to the proof due to Dinur in \cite{eurocrypt-2021-30841}, the necessary points depend on the hamming weight of the initial $n - n_1$ bits of the evaluation input for $\Tilde{\mathcal{P}}_k$. Dinur essentially states that in order to interpolate $U_0$ for some system $\Tilde{\mathcal{P}}_k$, all evaluations of $\Tilde{\mathcal{P}}_k$ on $(\hat{\mathbf{y}}, \hat{\mathbf{z}})$ are needed where the hamming weight of the $\hat{\mathbf{y}}$ vector is less than $w = d_{\Tilde{F}} - n_1$ (\cref{alg:solve:w} in \cref{alg:solve}). Likewise, to interpolate $U_i$, for $i = 1, \dots n_1$ all evaluations are needed where the hamming weight of the $\hat{\mathbf{y}}$-bits is less than $w + 1$.

To obtain evaluations of $\Tilde{\mathcal{P}}_k$, usable for the interpolation in \texttt{OUTPUT\_POTENTIALS}, Dinur specifies using the FES procedure (\cref{sec:prereq:fes}), denoted \texttt{BRUTEFORCE} at \cref{alg:uvalues:bruteforce} in \cref{alg:uvalues}. Using this procedure means that interpolation points can be obtained with time complexity 
$$
    2d \cdot \log n \cdot 2^{n_1} \cdot \binom{n - n_1}{\downarrow w}.
$$
However, as the FES procedure itself is not designed to iterate through sparse sets of inputs, such as $W^{n - n_1}_w \times \{0, 1\}^{n_1}$, an overhead is incurred. The $n_1$ latter bits can be iterated using the normal Gray code traversal of FES, meaning a penalty is incurred for each $2^{n_1}$ iteration. A conservative estimate by Dinur confines this to an amortized penalty of $2^{- n_1} \cdot n$ over the FES procedure. As the complexities derived for Dinur's polynomial method algorithm assume cryptographically relevant parameter sizes, $2^{n_1} \gg n$ means the overhead is negligible.

Once the relevant evaluations for $\Tilde{\mathcal{P}}_k$ have been computed, the procedure goes on to compute the actual interpolation points for the $U$ polynomials. The loop at \cref{alg:uvalues:sum} (\cref{alg:uvalues}) computes the evaluations for the $U$ polynomials by iterating through all evaluations and filtering depending on the values of the $\hat{\mathbf{y}}$ and $\hat{\mathbf{z}}$ bits. As the $U_0$ polynomial can be interpolated from the evaluations of $\hat{\mathcal{P}}_k$ in $W^{n - n_1}_w \times \{0, 1\}^{n - n_1}$, \cref{alg:uvalues} simply checks whether or not the current $\hat{\mathbf{y}}$ bits have a hamming weight $\leq w$, before summing and storing. Likewise, to filter out evaluations for some $U_i$, the procedure inspects the $\hat{\mathbf{z}}$-bits and checks whether or not $z_{i - 1} = 0$ (as is necessary by the construction of $U_i$s).

It should be noted that the Gray code traversal of the FES sub-procedure could also be reimplemented using monotone Gray codes instead of traditional binary reflected Gray codes, as noted by Dinur in \cite{eurocrypt-2021-30841}. This way, the Gray code traversal is done in an \textit{almost} increasing order according to the hamming weight.

\paragraph{Mitigating storage problems.} As is also noted by Dinur in \cite{eurocrypt-2021-30841}, computing several rounds in parallel can help mitigate some of the storage problems of the overall solver. In \cref{alg:output} exponentially sized lists/tables are computed and later stored, in order to check for \textit{candidate} solutions across rounds. If instead the $U$ polynomials for multiple rounds could be evaluated in parallel (or at least one after the other), their evaluations could be checked in parallel as well. There are multiple approaches for this, one of which is described in later sections and another described by Dinur in \cite{eurocrypt-2021-30841}. This does require certain modifications to the algorithm, like computing multiple $\Tilde{\mathcal{P}}_k$ systems in advance of calling \texttt{OUTPUT\_POTENTIALS()}. 